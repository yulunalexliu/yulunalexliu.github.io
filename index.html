<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yu-Lun Liu</title>
  
  <meta name="author" content="Yu-Lun Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/yulunliu_2_square.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yu-Lun (Alex) Liu | 劉育綸</name>
              </p>
              <p>
                <!-- I am a fifth year PhD student working with <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a> in the <a href="https://www.csie.ntu.edu.tw/">CSIE</a> department at <a href="https://www.ntu.edu.tw/">National Taiwan University</a>. I work on problems in computer vision, machine learning, and multimedia. -->
                I am an Assistant Professor in the <a href="https://www.cs.nycu.edu.tw/">Department of Computer Science</a> at <a href="https://www.nycu.edu.tw/">National Yang Ming Chiao Tung University</a>. I work on <b>image/video processing</b>, <b>computer vision</b>, and <b>computational photography</b>, particularly on essential problems requiring <b>machine learning</b> with insights from <b>geometry</b> and <b>domain-specific knowledge</b>.
              </p>
              <p>
                From 2014 to 2022, I was an algorithm development engineer at <a href="https://www.mediatek.tw/">MediaTek Inc.</a> and joined <a href="https://about.meta.com/realitylabs/">Meta Reality Labs Research</a> as a Research Scientist Intern in 2022. I received my Ph.D. from <a href="https://www.ntu.edu.tw/">National Taiwan University</a>, where I was advised by <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>. I did my undergrad and master's at <a href="https://www.nctu.edu.tw/">National Chiao Tung University</a>, where I worked with <a href="https://mcube.nctu.edu.tw/~hmhang/">Hsueh-Ming Hang</a>.
                
              </p>
        <!-- <p>I was a research scientist intern at Meta Reality Labs Research, where I worked on computational photography and dynamic novel view synthesis.
        </p>
        <p>I was also a senior algorithm development engineer at MediaTek Inc., where I worked on computational photography, computer vision, and machine learning.
        </p> -->
              <p style="text-align:center">
                <a href="mailto:yulunliu@cs.nycu.edu.tw">Email</a> &nbsp/&nbsp
                <a href="CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=gliihzoAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.facebook.com/profile.php?id=1837672300">Facebook</a> &nbsp/&nbsp
                <a href="https://www.instagram.com/alex04072000/">Instagram</a> &nbsp/&nbsp
                <a href="https://github.com/alex04072000">Github</a> &nbsp/&nbsp
                <a href="https://www.youtube.com/channel/UCpyNUU40gFm6_p3MVmFT01g">YouTube</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/yulunliu_2_square.jpg" class="hoverZoomLink">
              <!-- <img style="width:100%;max-width:100%" alt="profile photo"  onmouseover="document.getElementById('yulunliu').src='images/yulunliu_tired.jpg';document.getElementById('text-display').innerHTML='<center>Me, with a paper submission deadline approaching.</center>';"
                onmouseout="document.getElementById('yulunliu').src='images/yulunliu_2_square.jpg';document.getElementById('text-display').innerHTML='';"
                src="images/yulunliu_2_square.jpg" id="yulunliu">
                <div id="text-display" ></div> -->
            </td>
          </tr>
        </tbody></table>
        <b>I am looking for undergraduate / master's / Ph.D. / postdoc students to join my group. If you are interested in working with me and want to conduct research in image processing, computer vision, and machine learning, don't hesitate to contact me directly with your CV and transcripts.</b>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <center><heading>Research Group</heading></center>
              <p><center><subheading>MS Students</subheading></center></p>
              <div class="people">
                <div class="person">
                  <img src="images/avatar_scholar_256.png" />
                  <p>林晉暘<br>Institute of Data Science<br></p>
                </div>
                <div class="person">
                  <img src="images/avatar_scholar_256.png" />
                  <p>吳中赫<br>Institute of Multimedia<br></p>
                </div>
                <div class="person">
                  <img src="images/avatar_scholar_256.png" />
                  <p>嚴士函<br>Institute of Multimedia<br></p>
                </div>
                <div class="person">
                  <img src="images/avatar_scholar_256.png" />
                  <p>陳捷文<br>Institute of Computer Science<br></p>
                </div>
                <div class="person">
                  <img src="images/avatar_scholar_256.png" />
                  <p>鄭伯俞<br>Institute of Computer Science<br>(with <a href="https://walonchiu.github.io/">Wei-Chen Chiu</a>)</p>
                </div>
              </div>
              <p><center><subheading>BS Students</subheading></center></p>
              <div class="people">
                <div class="person">
                  <p>蘇智海<br>NYCU CS<br></p>
                </div>
                <div class="person">
                  <p>胡智堯<br>NTU MED<br></p>
                </div>
                <div class="person">
                  <p>陳俊瑋<br>NYCU MATH<br></p>
                </div>
                <div class="person">
                  <p>李明謙<br>NYCU ARETEHP<br></p>
                </div>
                <div class="person">
                  <p>林奕杰<br>NTHU SCIDM<br></p>
                </div>
                <div class="person">
                  <p>孫揚喆<br>NYCU MED<br></p>
                </div>
                <div class="person">
                  <p>羅宇呈<br>NYCU MATH<br></p>
                </div>
                <div class="person">
                  <p>郭玠甫<br>NYCU MATH<br></p>
                </div>
                <div class="person">
                  <p>葉柔昀<br>NYCU EP<br></p>
                </div>
                <div class="person">
                  <p>陳士弘<br>NYCU MATH<br></p>
                </div>
                <div class="person">
                  <p>鄭又豪<br>NYCU EP<br></p>
                </div>
                <div class="person">
                  <p>陳昱佑<br>NYCU CS<br></p>
                </div>
                <div class="person">
                  <p>丁祐承<br>NYCU MATH<br></p>
                </div>
                <div class="person">
                  <p>李宗諺<br>NYCU CS<br></p>
                </div>
                <div class="person">
                  <p>楊宗儒<br>NYCU CS<br></p>
                </div>
                <div class="person">
                  <p>陳凱昕<br>NYCU CS<br></p>
                </div>
                <div class="person">
                  <p>劉珆睿<br>NYCU CS<br></p>
                </div>
                <div class="person">
                  <p>吳俊宏<br>NYCU CS<br></p>
                </div>
                <div class="person">
                  <p>蔡師睿<br>NYCU CS<br></p>
                </div>
                <div class="person">
                  <p>張維程<br>NYCU CS<br></p>
                </div>
                <div class="person">
                  <p></p>
                </div>
                <div class="person">
                  <p></p>
                </div>
              </div>
            </td>
          </tr>
        </tbody></table>
        
        
        <!-- <li>林晉暘, 2023</li>
                <li>吳中赫, 2023</li>
                <li>嚴士函, 2023</li>
                <li>陳捷文, 2023</li>
                <li>鄭伯俞, 2023 (with <a href="https://walonchiu.github.io/">Wei-Chen Chiu</a>)</li>
              </ul>
            </td>
            <td style="padding:20px;width:33%;vertical-align:top">
              <subheading>Undergraduate Students</subheading>
              <br>
              <ul style="list-style:none; padding-left:0;">
                <li>蘇智海, 2023</li>
                <li>胡智堯, 2023</li>
                <li>陳俊瑋, 2023</li>
                <li>李明謙, 2023</li>
                <li>林奕杰, 2023</li>
                <li>孫揚喆, 2023</li>
                <li>羅宇呈, 2023</li>
                <li>郭玠甫, 2023</li>
                <li>葉柔昀, 2023</li>
                <li>陳士弘, 2023</li>
                <li>鄭又豪, 2023</li>
                <li>陳昱佑, 2023</li>
                <li>丁祐承, 2023</li>
                <li>李宗諺, 2023</li>
                <li>楊宗儒, 2023</li>
                <li>陳凱昕, 2023</li>
                <li>劉珆睿, 2023</li>
                <li>吳俊宏, 2023</li>
                <li>蔡師睿, 2023</li>
                <li>張維程, 2023</li> -->
        
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <center><heading>Research</heading></center>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <tr onmouseout="Meuleman2023POLstop()" onmouseover="Meuleman2023POL_start()"  bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
        <br>
        <br>
                <div class="two" id='Meuleman2023POL_image'><img src='images/Meuleman2023POL.gif' width="160"></div>
                <img src='images/Meuleman2023POL.jpg' width="160" height="90">
              </div>
              <script type="text/javascript">
                function Meuleman2023POL_start() {
                  document.getElementById('Meuleman2023POL_image').style.opacity = "1";
                }

                function Meuleman2023POLstop() {
                  document.getElementById('Meuleman2023POL_image').style.opacity = "0";
                }
                Meuleman2023POLstop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://localrf.github.io/">
                <papertitle>Progressively Optimized Local Radiance Fields for Robust View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://ameuleman.github.io/">Andreas Meuleman</a>, 
              <strong>Yu-Lun Liu</strong>, 
              <a href="http://chengao.vision/">Chen Gao</a>, 
              <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>, 
              <a href="https://changilkim.com/">Changil Kim</a>, 
              <a href="http://vclab.kaist.ac.kr/minhkim/">Min H. Kim</a>, 
              <a href="http://johanneskopf.de/">Johannes Kopf</a>
              <br>
              <em>CVPR</em>, 2023 &nbsp
              <br>
              <a href="https://localrf.github.io/">project page</a>
              /
              <a href="https://localrf.github.io/localrf.pdf">paper</a>
              /
              <a href="https://github.com/facebookresearch/localrf">code</a>
              /
              <a href="https://www.youtube.com/watch?v=GfXAHDxUY4M">video</a>
              <p></p>
              <p>
              For handling large unbounded scenes, we dynamically allocate new local radiance fields trained with frames within a temporal window.
            </td>
          </tr>

      
        <tr onmouseout="Liu2023RDR_stop()" onmouseover="Liu2023RDR_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
        <br>
        <br>
                <div class="two" id='Liu2023RDR_image'><img src='images/Liu2023RDR.gif' width="160"></div>
                <img src='images/Liu2023RDR.jpg' width="160" height="90">
              </div>
              <script type="text/javascript">
                function Liu2023RDR_start() {
                  document.getElementById('Liu2023RDR_image').style.opacity = "1";
                }

                function Liu2023RDR_stop() {
                  document.getElementById('Liu2023RDR_image').style.opacity = "0";
                }
                Liu2023RDR_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://robust-dynrf.github.io/">
                <papertitle>Robust Dynamic Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Yu-Lun Liu</strong>, 
              <a href="http://chengao.vision/">Chen Gao</a>, 
              <a href="https://ameuleman.github.io/">Andreas Meuleman</a>, 
              <a href="https://hytseng0509.github.io/">Hung-Yu Tseng</a>, 
              <a href="https://scholar.google.com/citations?user=bluhHm8AAAAJ&hl=en">Ayush Saraf</a>, 
              <a href="https://changilkim.com/">Changil Kim</a>, 
              <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
              <a href="http://johanneskopf.de/">Johannes Kopf</a>, 
              <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>
              <br>
              <em>CVPR</em>, 2023 &nbsp
              <br>
              <a href="https://robust-dynrf.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2301.02239">arXiv</a>
              /
              <a href="https://github.com/facebookresearch/robust-dynrf">code</a>
              /
              <a href="https://www.youtube.com/watch?v=38S56ottFQ4">video</a>
              <p></p>
              <p>
              RoDynRF tackles the robustness problem of conventional SfM systems such as COLMAP and showcases high-fidelity dynamic view synthesis results on a wide variety of videos.
            </td>
          </tr>
		

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
			    <br>
			    <br>
			    <br>
                <div class="two"><img src='images/iclr2022.jpg' width="160"></div>
                <img src='images/iclr2022.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.14206">
                <papertitle>Denoising Likelihood Score Matching for Conditional Score-based Data Generation</papertitle>
              </a>
              <br>
              Chen-Hao Chao, 
              <a href="https://scholar.google.com.tw/citations?user=TgMlVRUAAAAJ&hl=zh-TW">Wei-Fang Sun</a>, 
              Bo-Wun Cheng, 
              <a href="https://scholar.google.com/citations?user=EPYQ48sAAAAJ&hl=zh-TW">Yi-Chen Lo</a>, 
              <a href="https://scholar.google.com/citations?user=FK1RcpoAAAAJ&hl=zh-TW">Chia-Che Chang</a>, 
			        <strong>Yu-Lun Liu</strong>, 
              <a href="https://scholar.google.com/citations?user=0O9rukQAAAAJ&hl=en">Yu-Lin Chang</a>, 
              <a href="https://tw.linkedin.com/in/chia-ping-chen-81674078">Chia-Ping Chen</a>, 
              <a href="https://scholar.google.com/citations?user=5mYNdo0AAAAJ&hl=zh-TW">Chun-Yi Lee</a>
              <br>
              <em>ICLR</em>, 2022 &nbsp
              <br>
              <a href="https://arxiv.org/abs/2203.14206">arXiv</a>
              /
              <a href="https://openreview.net/forum?id=LcF-EEt8cCC">OpenReview</a>
              <p></p>
              <p>
              We theoretically formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density.
			</td>
          </tr>

		  
          <tr onmouseout="Liu2021SOLD_stop()" onmouseover="Liu2021SOLD_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
				<br>
				<br>
                <div class="two" id='Liu2021SOLD_image'><img src='images/cvpr20_reflection.gif' width="160"></div>
                <img src='images/pamiobstruction.jpg' width="160" height="90">
              </div>
              <script type="text/javascript">
                function Liu2021SOLD_start() {
                  document.getElementById('Liu2021SOLD_image').style.opacity = "1";
                }

                function Liu2021SOLD_stop() {
                  document.getElementById('Liu2021SOLD_image').style.opacity = "0";
                }
                Liu2021SOLD_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://alex04072000.github.io/SOLD/">
                <papertitle>Learning to See Through Obstructions with Layered Decomposition</papertitle>
              </a>
              <br>
			  <strong>Yu-Lun Liu</strong>, 
              <a href="https://www.wslai.net/">Wei-Sheng Lai</a>, 
              <a href="https://faculty.ucmerced.edu/mhyang/">Min-Hsuan Yang</a>, 
              <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
              <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a>
              <br>
              <em>TPAMI</em>, 2021 &nbsp
              <br>
              <a href="https://alex04072000.github.io/SOLD/">project page</a>
              /
              <a href="https://arxiv.org/abs/2008.04902">arXiv</a>
              /
              <a href="https://github.com/alex04072000/SOLD">code</a>
              /
              <a href="https://colab.research.google.com/drive/1kCG5SJd3usgzi6Bx979KiaO_YTanNVVz?usp=sharing">demo</a>
              /
              <a href="https://www.youtube.com/watch?v=oqdvYRYOT5s&t=4s">video</a>
              <p></p>
              <p>
              We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions, or adherent raindrops, from a short sequence of images captured by a moving camera.
            </td>
          </tr>
		
		  <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
			    <br>
			    <br>
			    <br>
                <div class="two"><img src='images/iccv2021.jpg' width="160"></div>
                <img src='images/iccv2021.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://albert100121.github.io/AiFDepthNet/">
                <papertitle>Bridging Unsupervised and Supervised Depth from Focus via All-in-Focus Supervision</papertitle>
              </a>
              <br>
              <a href="http://albert100121.github.io/">Ning-Hsu Wang</a>,
              <a href="https://tw.linkedin.com/in/ren-wang-61b273160">Ren Wang</a>,
			  <strong>Yu-Lun Liu</strong>, 
              <a href="https://www.linkedin.com/in/yu-hao-huang-72821060/">Yu-Hao Huang</a>, 
              <a href="https://scholar.google.com/citations?user=0O9rukQAAAAJ&hl=en">Yu-Lin Chang</a>, 
              <a href="https://tw.linkedin.com/in/chia-ping-chen-81674078">Chia-Ping Chen</a>, 
              <a href="https://dblp.org/pid/160/5531.html">Kevin Jou</a>
              <br>
              <em>ICCV</em>, 2021 &nbsp
              <br>
              <a href="https://albert100121.github.io/AiFDepthNet/">project page</a>
              /
              <a href="https://arxiv.org/abs/2108.10843">arXiv</a>
              /
              <a href="https://github.com/albert100121/AiFDepthNet">code</a>
              <p></p>
              <p>
              In this paper, we propose a method to estimate not only a depth map but an AiF image from a set of images with different focus positions (known as a focal stack).
			</td>
          </tr>
		
		  <tr onmouseout="Liu2021HNF_stop()" onmouseover="Liu2021HNF_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
				<br>
                <div class="two" id='Liu2021HNF_image'><br><img src='images/iccv21_nervis.gif' width="160"></div>
                <img src='images/FuSta.jpg' width="160" id='fusta_image'>
              </div>
              <script type="text/javascript">
                function Liu2021HNF_start() {
                  document.getElementById('Liu2021HNF_image').style.opacity = "1";
                  document.getElementById('fusta_image').style.opacity = "0";
                }

                function Liu2021HNF_stop() {
                  document.getElementById('Liu2021HNF_image').style.opacity = "0";
                  document.getElementById('fusta_image').style.opacity = "1";
                }
                Liu2021HNF_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://alex04072000.github.io/FuSta/">
                <papertitle>Hybrid Neural Fusion for Full-frame Video Stabilization</papertitle>
              </a>
              <br>
			  <strong>Yu-Lun Liu</strong>, 
              <a href="https://www.wslai.net/">Wei-Sheng Lai</a>, 
              <a href="https://faculty.ucmerced.edu/mhyang/">Min-Hsuan Yang</a>, 
              <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
              <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a>
              <br>
              <em>ICCV</em>, 2021 &nbsp
              <br>
              <a href="https://alex04072000.github.io/FuSta/">project page</a>
              /
              <a href="https://arxiv.org/abs/2102.06205">arXiv</a>
              /
              <a href="FuSta/iccv21_poster.pdf">poster</a>
              /
              <a href="FuSta/slides_mtk_25min.pptx">slides</a>
              /
              <a href="https://github.com/alex04072000/FuSta">code</a>
              /
              <a href="https://colab.research.google.com/drive/1l-fUzyM38KJMZyKMBWw_vu7ZUyDwgdYH?usp=sharing">demo</a>
              /
              <a href="https://www.youtube.com/watch?v=KO3sULs4hso">video</a>
              /
              <a href="https://www.youtube.com/watch?v=v5pOsQEOsyA">Two minute video</a>
              <p></p>
              <p>
              In this work, we present a frame synthesis algorithm to achieve full-frame video stabilization.
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
			    <br>
			    <br>
                <div class="two"><img src='images/icpr2020.png' width="160"></div>
                <img src='images/icpr2020.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2010.10000">
                <papertitle>Explorable Tone Mapping Operators</papertitle>
              </a>
              <br>
              Chien-Chuan Su, 
              <a href="https://tw.linkedin.com/in/ren-wang-61b273160">Ren Wang</a>,
              <a href="https://github.com/leVirve">Hung-Jin Lin</a>, 
			  <strong>Yu-Lun Liu</strong>, 
              <a href="https://tw.linkedin.com/in/chia-ping-chen-81674078">Chia-Ping Chen</a>, 
              <a href="https://scholar.google.com/citations?user=0O9rukQAAAAJ&hl=en">Yu-Lin Chang</a>, 
              <a href="https://scholar.google.com/citations?user=-JiGrnAAAAAJ">Soo-Chang Pei</a>
              <br>
              <em>ICPR</em>, 2020 &nbsp
              <br>
              <a href="https://arxiv.org/abs/2010.10000">arXiv</a>
              <p></p>
              <p>
              In this paper, a learning-based multimodal tone-mapping method is proposed, which not only achieves excellent visual quality but also explores the style diversity.
			</td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
			    <br>
                <div class="two"><img src='images/eccv2020.png' width="160"></div>
                <img src='images/eccv2020.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arcchang1236.github.io/CA-NoiseGAN/">
                <papertitle>Learning Camera-Aware Noise Models</papertitle>
              </a>
              <br>
              <a href="http://arcchang1236.github.io/">Ke-Chi Chang</a>,
              <a href="https://tw.linkedin.com/in/ren-wang-61b273160">Ren Wang</a>,
              <a href="https://github.com/leVirve">Hung-Jin Lin</a>, 
			  <strong>Yu-Lun Liu</strong>, 
              <a href="https://tw.linkedin.com/in/chia-ping-chen-81674078">Chia-Ping Chen</a>, 
              <a href="https://scholar.google.com/citations?user=0O9rukQAAAAJ&hl=en">Yu-Lin Chang</a>, 
              <a href="https://htchen.github.io/">Hwann-Tzong Chen</a>
              <br>
              <em>ECCV</em>, 2020 &nbsp
              <br>
              <a href="https://arcchang1236.github.io/CA-NoiseGAN">project page</a>
              /
              <a href="https://arxiv.org/abs/2008.09370">arXiv</a>
              /
              <a href="https://github.com/arcchang1236/CA-NoiseGAN">code</a>
              <p></p>
              <p>
              We propose a data-driven approach, where a generative noise model is learned from real-world noise.
			</td>
          </tr>

          <tr onmouseout="Liu2020SID_stop()" onmouseover="Liu2020SID_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
				<br>
                <div class="two" id='Liu2020SID_image'><img src='images/cvpr20_hdr.gif' width="160"></div>
                <img src='images/shdr.jpg' width="160">
              </div>
              <script type="text/javascript">
                function Liu2020SID_start() {
                  document.getElementById('Liu2020SID_image').style.opacity = "1";
                }

                function Liu2020SID_stop() {
                  document.getElementById('Liu2020SID_image').style.opacity = "0";
                }
                Liu2020SID_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://alex04072000.github.io/SingleHDR/">
                <papertitle>Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline</papertitle>
              </a>
              <br>
			  <strong>Yu-Lun Liu*</strong>, 
              <a href="https://www.wslai.net/">Wei-Sheng Lai*</a>, 
              <a href="https://www.cmlab.csie.ntu.edu.tw/~nothinglo/">Yu-Sheng Chen</a>, Yi-Lung Kao, 
              <a href="https://faculty.ucmerced.edu/mhyang/">Min-Hsuan Yang</a>, 
              <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
              <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a>
              <br>
              <em>CVPR</em>, 2020 &nbsp
              <br>
              <a href="https://alex04072000.github.io/SingleHDR/">project page</a>
              /
              <a href="https://arxiv.org/abs/2004.01179">arXiv</a>
              /
              <a href="SingleHDR_/cvpr20_singleHDR_poster_mtk.pdf">poster</a>
              /
              <a href="SingleHDR_/cvpr20_singleHDR_video_slides_15min.pptx">slides</a>
              /
              <a href="https://github.com/alex04072000/SingleHDR">code</a>
              /
              <a href="https://colab.research.google.com/drive/1WzNaGSaucF2AMDSdUCBMEOauBg4IowMa">demo</a>
              /
              <a href="https://www.youtube.com/watch?v=UCHDhk6fciY">1-minute video</a>
              <p></p>
              <p>
              In contrast to existing learning-based methods, our core idea is to incorporate the domain knowledge of the LDR image formation pipeline into our model.
            </td>
          </tr>

          <tr onmouseout="Liu2020LST_stop()" onmouseover="Liu2020LST_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
				<br>
				<br>
                <div class="two" id='Liu2020LST_image'><img src='images/cvpr20_reflection.gif' width="160"></div>
                <img src='images/reflection.jpg' width="160">
              </div>
              <script type="text/javascript">
                function Liu2020LST_start() {
                  document.getElementById('Liu2020LST_image').style.opacity = "1";
                }

                function Liu2020LST_stop() {
                  document.getElementById('Liu2020LST_image').style.opacity = "0";
                }
                Liu2020LST_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://alex04072000.github.io/ObstructionRemoval/">
                <papertitle>Learning to See Through Obstructions</papertitle>
              </a>
              <br>
			  <strong>Yu-Lun Liu</strong>, 
              <a href="https://www.wslai.net/">Wei-Sheng Lai</a>, 
              <a href="https://faculty.ucmerced.edu/mhyang/">Min-Hsuan Yang</a>, 
              <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
              <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a>
              <br>
              <em>CVPR</em>, 2020 &nbsp
              <br>
              <a href="https://alex04072000.github.io/ObstructionRemoval/">project page</a>
              /
              <a href="https://arxiv.org/abs/2004.01180">arXiv</a>
              /
              <a href="ObstructionRemoval_/cvpr20_obstructionRemoval_poster_mtk.pdf">poster</a>
              /
              <a href="ObstructionRemoval_/cvpr20_obstructionRemoval_video_slides_15min.pptx">slides</a>
              /
              <a href="https://github.com/alex04072000/ObstructionRemoval">code</a>
              /
              <a href="https://colab.research.google.com/drive/1iOKknc0dePekUH2TEh28EhcRPCS1mgwz">demo</a>
              /
              <a href="https://www.youtube.com/watch?v=pJWcHhofYTE">1-minute video</a>
              /
              <a href="https://www.youtube.com/watch?v=ICr6xi9wA94">video</a>
              /
              <a href="https://www.newscientist.com/article/2253195-ai-removes-unwanted-objects-from-photos-to-give-a-clearer-view/">New Scientists</a>
              <p></p>
              <p>
              We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions or raindrops, from a short sequence of images captured by a moving camera.
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
			    <br>
                <div class="two"><img src='images/AVS.jpg' width="160"></div>
                <img src='images/AVS.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Tsai2020AVS.pdf">
                <papertitle>Attention-based View Selection Networks for Light-field Disparity Estimation</papertitle>
              </a>
              <br>
			  Yu-Ju Tsai, 
			  <strong>Yu-Lun Liu</strong>, 
              <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
              <a href="https://www.csie.ntu.edu.tw/~ming/">Ming Ouhyoung</a>
              <br>
              <em>AAAI</em>, 2020 &nbsp
              <br>
              <a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Tsai2020AVS.pdf">paper</a>
              /
              <a href="https://github.com/LIAGM/LFattNet">code</a>
			  /
			  <a href="https://lightfield-analysis.uni-konstanz.de/benchmark/table">benchmark</a>
              <p></p>
              <p>
              For utilizing the views more effectively and reducing redundancy within views, we propose a view selection module that generates an attention map indicating the importance of each view and its potential for contributing to accurate depth estimation. 
            </td>
          </tr>
    
          <tr onmouseout="cyclic_stop()" onmouseover="cyclic_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cyclic_image'><img src='images/aaai-liu19.gif' width="160"></div>
				<br>
                <img src='images/cyclic.jpg' width="160">
              </div>
              <script type="text/javascript">
                function cyclic_start() {
                  document.getElementById('cyclic_image').style.opacity = "1";
                }

                function cyclic_stop() {
                  document.getElementById('cyclic_image').style.opacity = "0";
                }
                cyclic_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/CyclicGen">
                <papertitle>Deep Video Frame Interpolation using Cyclic Frame Generation</papertitle>
              </a>
              <br>
			  <strong>Yu-Lun Liu</strong>, 
              <a href="http://www.cmlab.csie.ntu.edu.tw/~queenieliaw/">Yi-Tung Liao</a>, 
              <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>, 
              <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>
              <br>
              <em>AAAI</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/CyclicGen">project page</a>
              /
              <a href="CyclicGen_/liu.pdf">paper</a>
              /
              <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/CyclicGen_/aaai19_cyclic_gen_poster.pdf">poster</a>
              /
              <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/CyclicGen_/CyclicGen_yylin_20190127.pptx">slides</a>
              /
              <a href="https://github.com/alex04072000/CyclicGen">code</a>
              /
              <a href="https://www.youtube.com/watch?v=R8vQjgAtPOE">video</a>
              <p></p>
              <p>
              The cycle consistency loss can better utilize the training data to not only enhance the interpolation results, but also maintain the performance better with less training data.
            </td>
          </tr>
		  
		  <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/14.png' width="160"></div>
                <img src='images/14.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7041517/">
                <papertitle>Background modeling using depth information</papertitle>
              </a>
              <br>
			  <strong>Yu-Lun Liu</strong>, 
              <a href="https://mcube.nctu.edu.tw/~hmhang/">Hsueh-Ming Hang</a>
              <br>
              <em>APSIPA</em>, 2014 &nbsp
              <br>
              <a href="http://cwww.ee.nctu.edu.tw/~hmhang/publications/Conf/Background%20Modeling%20Using%20Depth%20Information.pdf">paper</a>
              <p></p>
              <p>
              This paper mainly focuses on creating a global background model of a video sequence using the depth maps together with the RGB pictures.
            </td>
          </tr>
		  
		  <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/13.png' width="160"></div>
                <img src='images/13.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/6737719">
                <papertitle>Virtual view synthesis using backward depth warping algorithm</papertitle>
              </a>
              <br>
			  Du-Hsiu Li,
              <a href="https://mcube.nctu.edu.tw/~hmhang/">Hsueh-Ming Hang</a>,
			  <strong>Yu-Lun Liu</strong>
              <br>
              <em>PCS</em>, 2013 &nbsp
              <br>
              <a href="https://mcube.nctu.edu.tw/~hmhang/publications/Conf/Virtual%20View%20Synthesis%20Using%20Backward%20Depth%20Warping%20Algorithm.pdf">paper</a>
              <p></p>
              <p>
              In this study, we propose a backward warping process to replace the forward warping process, and the artifacts (particularly the ones produced by quantization) are significantly reduced.
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <center><heading>Teaching</heading></center>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/NYCU.svg.png" width="160"></td>
            <td width="75%" valign="center">
              CSIC30107 (Video Compression): <a href="https://timetable.nycu.edu.tw/?r=main/crsoutline&Acy=111&Sem=2&CrsNo=535659&lang=zh-tw">Spring 2023</a>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <center><heading>Sponsors</heading></center>
              <p>My research is made possible by the generous support of the following organizations.
                <!-- three columes for images, each 33% of the talbe size -->
                <!-- columes! not rows! -->
                <!-- <div class="container">
                  <div class="row">
                <div class="col-xs-2 col-sm-6 col-md-4" align="center"><img src="images/2880px-MediaTek_logo.svg.png" width="160"></div>
                <div class="col-xs-2 col-sm-6 col-md-4" align="center"><img src="images/ROC_National_Science_and_Technology_Council.svg.png" width="160"></div>
                <div class="col-xs-2 col-sm-6 col-md-4" align="center"><img src="images/2880px-Industrial_Technology_Research_Institute_logo.svg.png" width="160"></div>
              </div>
            </div> -->
            <div class="row centered">
              <div class="sponsor">
                <p><img src="images/2880px-MediaTek_logo.svg.png" width="160"></p>
              </div>
              <div class="sponsor">
                <p><img src="images/NVIDIA_logo.svg.png" width="160"></p>
              </div>
              <div class="sponsor">
                <p><img src="images/ROC_National_Science_and_Technology_Council.svg.png" width="160"></p>
              </div>
              <div class="sponsor">
                <p><img src="images/2880px-Industrial_Technology_Research_Institute_logo.svg.png" width="160"></p>
              </div>
            </div>


                <!-- <div style="width: 25%; text-align: center;"><img src="images/2880px-MediaTek_logo.svg.png"></div>
                <div style="width: 25%; text-align: center;"><img src="images/2880px-MediaTek_logo.svg.png"></div>
                <div style="width: 25%; text-align: center;"><img src="images/2880px-MediaTek_logo.svg.png"></div> -->
                <!-- <table><tr><td><img height=50 src="images/2880px-MediaTek_logo.svg.png"></td><td><img height=50 src="images/ROC_National_Science_and_Technology_Council.svg.png"></td><td><img height=50 src="images/2880px-Industrial_Technology_Research_Institute_logo.svg.png"></td></tr></table></p> -->
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
        Stolen from <a href="https://jonbarron.info/">Jon Barron</a>'s website.
                <br>
                Last updated July 2023.
        </p>
            </td>
          </tr>
        </tbody></table>

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              Reviewer: 
              <br>
              SIGGRAPH 2022
              <br>
              CVPR 2022
              <br>
              ICCV 2021
              <br>
              ECCV 2020-2022
              <br>
              ACCV 2020
              <br>
              ICLR 2022
              <br>
              NeurIPS 2022
              <br>
              AAAI 2021-2022
              <br>
              IJCAI 2021-2022
              <br>
              MM 2018
              <br>
              TIP
              <br>
              Applied Soft Computing
              <br>
              Pattern Recognition
              <br>
              CVIU
              <br>
              Multidimensional Systems and Signal Processing
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
        Stolen from <a href="https://jonbarron.info/">Jon Barron</a>'s website.
                <br>
                Last updated Feburary 2023.
        </p>
            </td>
          </tr>
        </tbody></table> -->
      </td>
    </tr>
  </table>
</body>

</html>
