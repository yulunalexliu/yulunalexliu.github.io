<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yu-Lun Liu</title>
    <meta name="author" content="Yu-Lun Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü¶ä</text></svg>">
    <script src="js/hidebib.js" type="text/javascript"></script>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.2/font/bootstrap-icons.min.css" />

</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr style="padding:0px">
                      <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                          <name>Yu-Lun (Alex) Liu | ÂäâËÇ≤Á∂∏</name>
                        </p>
                        <p style="text-align:justify;">
                          <!-- I am a fifth year PhD student working with <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a> in the <a href="https://www.csie.ntu.edu.tw/">CSIE</a> department at <a href="https://www.ntu.edu.tw/">National Taiwan University</a>. I work on problems in computer vision, machine learning, and multimedia. -->
                          I am an Assistant Professor in the <a href="https://www.cs.nycu.edu.tw/">Department of Computer Science</a> at <a href="https://www.nycu.edu.tw/">National Yang Ming Chiao Tung University</a>. I work on <b>image/video processing</b>, <b>computer vision</b>, and <b>computational photography</b>, particularly on essential problems requiring <b>machine learning</b> with insights from <b>geometry</b> and <b>domain-specific knowledge</b>.
                        </p>
                        <p style="text-align:justify;">
                          <!-- I am a fifth year PhD student working with <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a> in the <a href="https://www.csie.ntu.edu.tw/">CSIE</a> department at <a href="https://www.ntu.edu.tw/">National Taiwan University</a>. I work on problems in computer vision, machine learning, and multimedia. -->
                          Prior to joining NYCU, I was a Research Scientist Intern at <a href="https://about.meta.com/realitylabs/">Meta Reality Labs Research</a> and a senior software engineer at <a href="https://www.mediatek.tw/">MediaTek Inc</a>. I received my PhD from <a href="https://www.csie.ntu.edu.tw/">NTU, CSIE</a> in 2022, where I was a member of <a href="https://www.cmlab.csie.ntu.edu.tw/new_cml_website/index.php">CMLab</a>.
                        </p>
                        <p style="text-align:justify;">
                          I received the <a href="https://yushan.project.edu.tw/TopTalent">ÊïôËÇ≤ÈÉ®ÁéâÂ±±ÈùíÂπ¥Â≠∏ËÄÖ</a>, <a href="https://www.nstc.gov.tw/folksonomy/list/b783c8f8-92fd-444b-a3dd-c9f026124319?l=ch">ÂúãÁßëÊúÉ 2030 Êñ∞ÁßÄÂ≠∏ËÄÖ</a>, <a href="https://research.google/programs-and-events/research-scholar-program/recipients/">Google Research Scholar Award</a>, and <a href="https://x.com/CVPR/status/1793616950314369239">CVPR 2024 Outstanding Reviewer</a> award.
                        </p>
                        <p style="text-align:justify;">
                        <b>Dear prospective students:</b><i> I am looking for undergraduate / master's / Ph.D. / postdoc students to join my group. If you are interested in working with me and want to conduct research in image processing, computer vision, and machine learning, don't hesitate to contact me directly with your CV and transcripts.</i>
                        </p>
                        <!-- <p style="text-align:justify;">
                          From 2014 to 2022, I was an algorithm development engineer at <a href="https://www.mediatek.tw/">MediaTek Inc.</a> and joined <a href="https://about.meta.com/realitylabs/">Meta Reality Labs Research</a> as a Research Scientist Intern in 2022. I received my Ph.D. from <a href="https://www.ntu.edu.tw/">National Taiwan University</a>, where I was advised by <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>. I did my undergrad and master's at <a href="https://www.nctu.edu.tw/">National Chiao Tung University</a>, where I worked with <a href="https://mcube.nctu.edu.tw/~hmhang/">Hsueh-Ming Hang</a>.
                          
                        </p> -->
                  <!-- <p>I was a research scientist intern at Meta Reality Labs Research, where I worked on computational photography and dynamic novel view synthesis.
                  </p>
                  <p>I was also a senior algorithm development engineer at MediaTek Inc., where I worked on computational photography, computer vision, and machine learning.
                  </p> -->
                        <p style="text-align:center">
                          <a href="mailto:yulunliu@cs.nycu.edu.tw">Email</a> &nbsp/&nbsp
                          <a href="CV.pdf">CV</a> &nbsp/&nbsp
                          <a href="https://scholar.google.com/citations?user=gliihzoAAAAJ">Google Scholar</a> &nbsp/&nbsp
                          <a href="https://www.facebook.com/profile.php?id=1837672300">Facebook</a> &nbsp/&nbsp
                          <a href="https://www.instagram.com/alex04072000/">Instagram</a> &nbsp/&nbsp
                          <a href="https://github.com/alex04072000">Github</a> &nbsp/&nbsp
                          <a href="https://www.youtube.com/channel/UCpyNUU40gFm6_p3MVmFT01g">YouTube</a>
                        </p>
                      </td>
                      <td style="padding:2.5%;width:40%;max-width:40%">
                        <!-- <img style="width:100%;max-width:100%" alt="profile photo" src="images/yulunliu_2_square.jpg" class="hoverZoomLink">
                        For those who personally know me, I know what you're thinking: Who is this guy? <br> Hover over to see what I usually look like before a paper submission deadline. -->
                        <!-- <img style="width:100%;max-width:100%" alt="profile photo"  onmouseover="document.getElementById('yulunliu').src='images/yulunliu_tired.jpg';document.getElementById('text-display').innerHTML='<center>Me, with a paper submission deadline approaching.</center>';"
                          onmouseout="document.getElementById('yulunliu').src='images/yulunliu_2_square.jpg';document.getElementById('text-display').innerHTML='';"
                          src="images/yulunliu_2_square.jpg" id="yulunliu">
                          <div id="text-display" ></div> -->
                        <img style="width:100%;max-width:100%" alt="profile photo"  onmouseover="document.getElementById('yulunliu').src='images/yulunliu_tired.jpg';"
                        onmouseout="document.getElementById('yulunliu').src='images/yulunliu_2_square.jpg';document.getElementById('text-display').innerHTML='';"
                        src="images/yulunliu_2_square.jpg" id="yulunliu">
                        For those who personally know me, that might be thinking: Who is this guy? <br> Hover over to see how I usually look like before a paper submission deadline.
                        <br><br>Meet <a href="images/both.jpg">my cats</a>: <a href="images/tiger.jpg">ËôéÁöÆÊç≤</a> and <a href="images/chicken.jpg">ÈõûËõãÁ≥ï</a>!
                      </td>
                    </tr>
                  </tbody></table>
                  <!-- <div class="container">
                  <p style="text-align:justify;">
                    <i>I am looking for undergraduate / master's / Ph.D. / postdoc students to join my group. If you are interested in working with me and want to conduct research in image processing, computer vision, and machine learning, don't hesitate to contact me directly with your CV and transcripts.</i>
                  </p>
                  </div> -->
          
                  <hr class="soft">
                  <br>
                  <div class="container quote2">
                    <center><heading>Timeline</heading></center>
                  </div>
                  <div class="container">
                    <div id="timeline">
                      <div class="timelineitem">
                        <div class="tdate"> 2023 - </div>
                        <img src="images/National_Yang_Ming_Chiao_Tung_University_seal_(2023).svg.png" alt="image" height="80"/>
                        <div class="ttitle">Assistant Professor at NYCU</div>
                      <div class="tdesc">Comp Photo Lab, <a href="https://www.cs.nycu.edu.tw/">Department of Computer Science</a></div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2022</div>
                        <img src="images/Meta_Platforms_Inc._logo.svg.png" alt="image" height="36"/>
                        <div class="ttitle">Research Scientist Intern at Meta</div>
                      <div class="tdesc"> Worked with <a href="https://jbhuang0604.github.io/">Prof. Jia-Bin Huang</a> and <a href="http://johanneskopf.de/">Dr. Johannes Kopf</a></div>
                      <div class="tdesc">Computational Photography Group</div>
                      <div class="tdesc">Seattle, WA, USA</div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2017 - 2022</div>
                        <img src="images/2880px-MediaTek_logo.svg.png" alt="image" height="41"/>
                        <div class="ttitle">Senior Software Engineer at MediaTek Inc.</div>
                      <div class="tdesc">Multimedia Technology Development (MTD) Division</div>
                      <div class="tdesc">Intelligent Vision Processing (IVP) Department</div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2017 - 2022</div>
                        <img src="images/500px-National_Taiwan_University_logo.svg.png" alt="image" height="80"/>
                        <div class="ttitle">NTU: PhD</div>
                        <!-- <div class="tdesc"><a href="https://www.cmlab.csie.ntu.edu.tw/new_cml_website/index.php">CMLab</a>, Department of Computer Science & Information Engineering (<a href="https://www.csie.ntu.edu.tw/">CSIE</a>)</div> -->
                        <div class="tdesc">CMLab, CSIE</div>
                        <div class="tdesc">Worked with <a href="https://www.csie.ntu.edu.tw/~cyy/">Prof. Yung-Yu Chuang</a></div>
                        <div class="tdesc">Thesis: <a href="https://hdl.handle.net/11296/8zg4aq">Restoring and Enhancing Images and Videos by Combining Modeling and Learning</a></div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2014 - 2017</div>
                        <img src="images/2880px-MediaTek_logo.svg.png" alt="image" height="41"/>
                        <div class="ttitle">Software Engineer at MediaTek Inc.</div>
                      <div class="tdesc">Multimedia Technology Development (MTD) Division</div>
                      <div class="tdesc">Intelligent Vision Processing (IVP) Department</div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2012 - 2014</div>
                        <img src="images/NCTU_emblem.png" alt="image" height="80"/>
                        <div class="ttitle">NCTU: MS</div>
                        <!-- <div class="tdesc"><a href="https://mcube.lab.nycu.edu.tw/">CommLab</a>, <a href="https://eenctu.nctu.edu.tw/index.php">Institute of Electronics</a></div> -->
                        <div class="tdesc">CommLab, Institute of Electronics</div>
                        <div class="tdesc">Worked with <a href="https://mcube.lab.nycu.edu.tw/~hmhang/">Prof. Hsueh-Ming Hang</a></div>
                        <div class="tdesc">Thesis: <a href="https://hdl.handle.net/11296/889ms7">Wide-Angle Virtual View Synthesis with Depth-Based Background Modeling</a></div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2008 - 2012</div>
                        <img src="images/NCTU_emblem.png" alt="image" height="80"/>
                        <div class="ttitle">NCTU: BS</div>
                        <div class="tdesc">Department of Electronics Engineering</div>
                      </div>
                      <!-- <div class="timelineitem">
                        <div class="tdate">2017 - 2018</div>
                        <div class="ttitle">PostDoc at TU Graz</div>
                      <div class="tdesc"> Institute of Electrical Measurement and Sensor Systems (<a href="https://www.tugraz.at/en/institutes/emt/home/">EMS</a>), Graz, Austria <span class="thigh"> </span> </div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2014 - 2017</div>
                        <div class="ttitle">Visiting Researcher at York University </div>
                      <div class="tdesc"> Worked with <span class="thigh">Prof. Richard P. Wildes </span> </div>
                      <div class="tdesc"> YorkU <a href="http://vision.eecs.yorku.ca/main/">Vision Lab</a>, Toronto, Canada</div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2014 - 2017</div>
                        <div class="ttitle">TU Graz: PhD</div>
                        <div class="tdesc">Thesis: <span class="thigh">Deep Learning for Video Recognition</span></div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2012 - 2013</div>
                        <div class="ttitle">TU Graz: MSc </div>
                        <div class="tdesc">Thesis: <span class="thigh"><a href="pubs/Feichtenhofer_MScThesis_5Nov13.pdf">Dynamic Scene Recognition with Oriented Spacetime Energies</a> </span></div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2013</div>
                      <div class="ttitle">Visiting Researcher at York University </div>
                      <div class="tdesc"> Worked with <span class="thigh">Prof. Richard P. Wildes </span> </div>
                      <div class="tdesc"> YorkU <a href="http://vision.eecs.yorku.ca/main/">Vision Lab</a>, Toronto, Canada</div>
                      </div>
                      <div class="timelineitem">
                        <div class="tdate">2008 - 2011
                        </div>
                        <div class="ttitle">TU Graz: BSc </div>
                        <div class="tdesc">Thesis: <span class="thigh"><a href="pubs/BSc_thesis_feichtenhofer_sharpness_10-2011.pdf">No-Reference Sharpness Metric based on Local Gradient Analysis</a> </span></div>
                      </div> -->
                    </div>
                  </div>
          
                  <hr class="soft">
          
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <center><heading>News</heading></center>
                        <ul>
                          <!-- <li>Sep 2024: Three conference papers accepted to <a href="https://neurips.cc/">NeurIPS 2024</a></li>
                          <li>Sep 2024: Talk at NYCU Library</li>
                          <li>Aug 2024: MS student ÈÑ≠‰ºØ‰øû won the <a href="https://ippr.org.tw/wp-content/uploads/2024/08/%E7%AC%AC17%E5%B1%86%E5%8D%9A%E7%A2%A9%E5%A3%AB%E8%AB%96%E6%96%87%E7%8D%8E%E7%8D%B2%E7%8D%8E%E5%90%8D%E5%96%AE.pdf">‰∏≠ËèØÊ∞ëÂúãÂΩ±ÂÉèËôïÁêÜËàáÂúñÂΩ¢Ë≠òÂà•Â≠∏ÊúÉÁ¢©Â£´‰Ω≥‰ΩúË´ñÊñáÁçé (IPPR Excellent Master Thesis Award)</a>!</li>
                          <li>July 2024: BS student <a href="https://jayinnn.dev/">ÊùéÊù∞Á©é</a> won the <a href="https://www.cs.nycu.edu.tw/storage/materials/xeXTWKdsG4IkteKZGx3lxO6WdeZv4Qi0mgaomFJr.pdf">ÈôΩÊòé‰∫§Â§ßË≥áÂ∑•Á≥ªÂ∞àÈ°åÁ´∂Ë≥Ω‰Ω≥‰Ωú (3rd place of the NYCU CS Undergraduate Research Competition)</a>!</li>
                          <li>July 2024: BS students Âê≥‰øäÂÆè, Èô≥Â£´Âºò, Èô≥Âá±Êòï, and Èô≥Êò±‰Ωë won the <a href="https://www.cs.nycu.edu.tw/storage/materials/xeXTWKdsG4IkteKZGx3lxO6WdeZv4Qi0mgaomFJr.pdf">ÈôΩÊòé‰∫§Â§ßË≥áÂ∑•Á≥ªÂ∞àÈ°åÁ´∂Ë≥ΩÂÑ™Á≠â (2nd place of the NYCU CS Undergraduate Research Competition)</a>!</li>
                          <li>July 2024: One conference paper accepted to <a href="https://eccv.ecva.net/">ECCV 2024</a></li>
                          <li>June 2024: One conference paper accepted to <a href="https://iros2024-abudhabi.org/">IROS 2024</a></li>
                          <li>June 2024: Excited and honored to have been selected as a <a href="https://www.nstc.gov.tw/folksonomy/list/b783c8f8-92fd-444b-a3dd-c9f026124319?l=ch">2030 Êñ∞ÁßÄÂ≠∏ËÄÖ (NSTC 2030 Emerging Young Scholar)</a>!</li>
                          <li>June 2024: BS students <a href="https://su-terry.github.io/">ËòáÊô∫Êµ∑</a> and <a href="https://jayinnn.dev/">ÊùéÊù∞Á©é</a> are awarded the <a href="https://www.nstc.gov.tw/folksonomy/list/2af9ad9a-1f47-450d-b5a1-2cb43de8290c?l=ch">ÂúãÁßëÊúÉÂ§ßÂ∞àÂ≠∏ÁîüÁ†îÁ©∂Ë®àÁï´ (NSTC Research Grant for University Students)</a>!</li>
                          <li>May 2024: Honored to win a CVPR 2024 <a href="https://x.com/CVPR/status/1793616950314369239/photo/1">Outstanding Reviewer</a> award!</li>
                          <li>April 2024: Excited and honored to receive a <a href="https://research.google/programs-and-events/research-scholar-program/recipients/">Google Research Scholar Award</a>!</li>
                          <li>Mar 2024: Two conference papers conditionally accepted to <a href="https://s2024.siggraph.org/">SIGGRAPH 2024</a></li>
                          <li>Mar 2024: Invited <a href="https://csie.ntu.edu.tw/zh_tw/Announcements/Announcement7/%5B2024-03-01%5D%C2%A0Prof-Yu-Lun-Alex-Liu-National-Yang-Ming-Chiao-Tung-University-Enhancing-3D-Scene-Reconstruction-in-NeRFs-36521134">Seminar</a> at NTU</li>
                          <li>Feb 2024: Two conference papers accepted to <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a></li>
                          <li>Feb 2024: Keynote at <a href="http://mislab.cs.nthu.edu.tw/explorecsr-3/index.html#program">2024 Fun AI Winter Camp</a></li>
                          <li>Jan 2024: BS student <a href="https://su-terry.github.io/">ËòáÊô∫Êµ∑</a> won the <a href="https://www.cs.nycu.edu.tw/storage/materials/BqI0kDw9wXf5rwQmjVkOkBbVNEOKO7w3QvyOUVtQ.pdf">ÈôΩÊòé‰∫§Â§ßË≥áÂ∑•Á≥ªÂ∞àÈ°åÁ´∂Ë≥ΩÁâπÂÑ™ (1st place of the NYCU CS Undergraduate Research Competition)</a>!</li>
                          <li>Jan 2024: One conference paper accepted to <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a></li>
                          <li>Jan 2024: One Journal paper accepted to <a href="https://link.springer.com/journal/11263">IJCV</a> 2024</li>
                          <li>Dec 2023: One conference paper accepted to <a href="https://aaai.org/aaai-conference/">AAAI 2024</a></li>
                          <li>Aug 2023: Honored to receive Google's Research Grant!</li>
                          <li>Aug 2023: Honored to receive the <a href="https://140.125.183.142/wp-content/uploads/2023/08/%E7%AC%AC16%E5%B1%86%E5%8D%9A%E7%A2%A9%E5%A3%AB%E8%AB%96%E6%96%87%E7%8D%8E%E7%8D%B2%E7%8D%8E%E5%90%8D%E5%96%AE.pdf">‰∏≠ËèØÊ∞ëÂúãÂΩ±ÂÉèËôïÁêÜËàáÂúñÂΩ¢Ë≠òÂà•Â≠∏ÊúÉÂçöÂ£´ÂÑ™Á≠âË´ñÊñáÁçé (IPPR Best Ph.D. Thesis Award)</a>!</li>
                          <li>July 2023: Excited and honored to have been selected as a <a href="https://yushan.moe.gov.tw/TopTalent">ÁéâÂ±±ÈùíÂπ¥Â≠∏ËÄÖ</a> (<a href="https://yushan.moe.gov.tw/TopTalent/EN">Yushan Young Fellow</a>), 2023!</li>
                          <li>July 2023: Two conference papers accepted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a></li>
                          <li>May 2023: Talk at MediaTek Inc.</li>
                          <li>April 2023: Invited Seminar at NTHU</li>
                          <li>Mar 2023: Talk at the <a href="https://elsa-lab.github.io/AIIWorkshop_5th/index.html">5th AII Workshop</a></li>
                          <li>Mar 2023: Invited Seminar at NYCU</li>
                          <li>Mar 2023: Two conference papers accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a></li>
                          <li>Jan 2023: Talk at Academia Sinica</li>
                          <li>Nov 2022: Invited Seminar at NTHU</li> -->
                          <li>Dec 2024: Honored to receive the <a href="https://www.cs.nycu.edu.tw/announcements/detail/12172">ÈôΩÊòé‰∫§Â§ßË≥áË®äÂ≠∏Èô¢Ë≥áË®äÂπ¥ËºïÂ≠∏ËÄÖÂçìË∂äË≤¢ÁçªÁçé</a>!</li>
                          <li>Nov 2024: MS student ÈÑ≠‰ºØ‰øû won the <a href="https://taai2024.org/113%E5%B9%B4-%E7%A2%A9%E5%8D%9A%E5%A3%AB%E8%AB%96%E6%96%87%E7%8D%8E-%E5%BE%97%E7%8D%8E%E5%90%8D%E5%96%AE/">Á¨¨ 29 Â±Ü‰∫∫Â∑•Êô∫ÊÖßËàáÊáâÁî®Á†îË®éÊúÉ TAAI 2024 Á¢©Â£´Ë´ñÊñá‰Ω≥‰ΩúÁçé</a>!</li>
                          <li>Sep 2024: Three conference papers accepted to <a href="https://neurips.cc/">NeurIPS 2024</a></li>
                          <li>Sep 2024: <a href="https://www.youtube.com/watch?v=Xt17-D9Mp9A">Talk</a> at NYCU Library</li>
                          <li>Aug 2024: MS student ÈÑ≠‰ºØ‰øû won the <a href="https://ippr.org.tw/wp-content/uploads/2024/08/%E7%AC%AC17%E5%B1%86%E5%8D%9A%E7%A2%A9%E5%A3%AB%E8%AB%96%E6%96%87%E7%8D%8E%E7%8D%B2%E7%8D%8E%E5%90%8D%E5%96%AE.pdf">‰∏≠ËèØÊ∞ëÂúãÂΩ±ÂÉèËôïÁêÜËàáÂúñÂΩ¢Ë≠òÂà•Â≠∏ÊúÉÁ¢©Â£´‰Ω≥‰ΩúË´ñÊñáÁçé</a>!</li>
                          <li>July 2024: BS student <a href="https://jayinnn.dev/">ÊùéÊù∞Á©é</a> won the <a href="https://www.cs.nycu.edu.tw/storage/materials/xeXTWKdsG4IkteKZGx3lxO6WdeZv4Qi0mgaomFJr.pdf">ÈôΩÊòé‰∫§Â§ßË≥áÂ∑•Á≥ªÂ∞àÈ°åÁ´∂Ë≥Ω‰Ω≥‰Ωú</a>!</li>
                          <li>July 2024: BS students Âê≥‰øäÂÆè, Èô≥Â£´Âºò, Èô≥Âá±Êòï, and Èô≥Êò±‰Ωë won the <a href="https://www.cs.nycu.edu.tw/storage/materials/xeXTWKdsG4IkteKZGx3lxO6WdeZv4Qi0mgaomFJr.pdf">ÈôΩÊòé‰∫§Â§ßË≥áÂ∑•Á≥ªÂ∞àÈ°åÁ´∂Ë≥ΩÂÑ™Á≠â</a>!</li>
                          <li>July 2024: One conference paper accepted to <a href="https://eccv.ecva.net/">ECCV 2024</a></li>
                          <li>June 2024: One conference paper accepted to <a href="https://iros2024-abudhabi.org/">IROS 2024</a></li>
                          <li>June 2024: Excited and honored to have been selected as a <a href="https://www.nstc.gov.tw/folksonomy/list/b783c8f8-92fd-444b-a3dd-c9f026124319?l=ch">ÂúãÁßëÊúÉ 2030 Êñ∞ÁßÄÂ≠∏ËÄÖ</a>!</li>
                          <li>June 2024: BS students <a href="https://su-terry.github.io/">ËòáÊô∫Êµ∑</a> and <a href="https://jayinnn.dev/">ÊùéÊù∞Á©é</a> are awarded the <a href="https://www.nstc.gov.tw/folksonomy/list/2af9ad9a-1f47-450d-b5a1-2cb43de8290c?l=ch">ÂúãÁßëÊúÉÂ§ßÂ∞àÂ≠∏ÁîüÁ†îÁ©∂Ë®àÁï´</a>!</li>
                          <li>May 2024: Honored to win a CVPR 2024 <a href="https://x.com/CVPR/status/1793616950314369239/photo/1">Outstanding Reviewer</a> award!</li>
                          <li>April 2024: Excited and honored to receive a <a href="https://research.google/programs-and-events/research-scholar-program/recipients/">Google Research Scholar Award</a>!</li>
                          <li>Mar 2024: Two conference papers conditionally accepted to <a href="https://s2024.siggraph.org/">SIGGRAPH 2024</a></li>
                          <li>Mar 2024: Invited <a href="https://csie.ntu.edu.tw/zh_tw/Announcements/Announcement7/%5B2024-03-01%5D%C2%A0Prof-Yu-Lun-Alex-Liu-National-Yang-Ming-Chiao-Tung-University-Enhancing-3D-Scene-Reconstruction-in-NeRFs-36521134">Seminar</a> at NTU</li>
                          <li>Feb 2024: Two conference papers accepted to <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a></li>
                          <li>Feb 2024: Keynote at <a href="http://mislab.cs.nthu.edu.tw/explorecsr-3/index.html#program">2024 Fun AI Winter Camp</a></li>
                          <li>Jan 2024: BS student <a href="https://su-terry.github.io/">ËòáÊô∫Êµ∑</a> won the <a href="https://www.cs.nycu.edu.tw/storage/materials/BqI0kDw9wXf5rwQmjVkOkBbVNEOKO7w3QvyOUVtQ.pdf">ÈôΩÊòé‰∫§Â§ßË≥áÂ∑•Á≥ªÂ∞àÈ°åÁ´∂Ë≥ΩÁâπÂÑ™</a>!</li>
                          <li>Jan 2024: One conference paper accepted to <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a></li>
                          <li>Jan 2024: One Journal paper accepted to <a href="https://link.springer.com/journal/11263">IJCV</a> 2024</li>
                          <li>Dec 2023: One conference paper accepted to <a href="https://aaai.org/aaai-conference/">AAAI 2024</a></li>
                          <li>Aug 2023: Honored to receive Google's Research Grant!</li>
                          <li>Aug 2023: Honored to receive the <a href="https://140.125.183.142/wp-content/uploads/2023/08/%E7%AC%AC16%E5%B1%86%E5%8D%9A%E7%A2%A9%E5%A3%AB%E8%AB%96%E6%96%87%E7%8D%8E%E7%8D%B2%E7%8D%8E%E5%90%8D%E5%96%AE.pdf">‰∏≠ËèØÊ∞ëÂúãÂΩ±ÂÉèËôïÁêÜËàáÂúñÂΩ¢Ë≠òÂà•Â≠∏ÊúÉÂçöÂ£´ÂÑ™Á≠âË´ñÊñáÁçé</a>!</li>
                          <li>July 2023: Excited and honored to have been selected as a <a href="https://yushan.moe.gov.tw/TopTalent">ÊïôËÇ≤ÈÉ®ÁéâÂ±±ÈùíÂπ¥Â≠∏ËÄÖ</a>!</li>
                          <li>July 2023: Two conference papers accepted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a></li>
                          <li>May 2023: Talk at MediaTek Inc.</li>
                          <li>April 2023: Invited Seminar at NTHU</li>
                          <li>Mar 2023: Talk at the <a href="https://elsa-lab.github.io/AIIWorkshop_5th/index.html">5th AII Workshop</a></li>
                          <li>Mar 2023: Invited Seminar at NYCU</li>
                          <li>Mar 2023: Two conference papers accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a></li>
                          <li>Jan 2023: Talk at Academia Sinica</li>
                          <li>Nov 2022: Invited Seminar at NTHU</li>
                        </ul>
                      </td>
                    </tr>
                  </tbody></table>
          
                  <hr class="soft">
          
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <p><center><heading>Research Group</heading></center></p>
                        <div id="slideshow">
                          <figure>
                              <img src="images/20230429.jpg">
                              <img src="images/20231208.jpg">
                              <img src="images/20240121.jpg">
                              <img src="images/20240402.jpg">
                              <img src="images/20240428.jpg">
                          </figure>
                        </div>
                        <p><center><subheading>Postdoc</subheading></center></p>
                        <div class="people">
                          <div class="person">
                            <img src="students/zhenjunzhao.jpg" />
                            <p><a href="https://ericzzj1989.github.io/">ËµµÁ•Ø‰øä</a><br>PhD, CUHK<br></p>
                          </div>
                        </div>
                        <p><center><subheading>PhD Students</subheading></center></p>
                        <div class="people">
                          <div class="person">
                            <img src="students/yichuanhuang.jpg" />
                            <p>ÈªÉÊÄ°Â∑ù<br>Institute of<br>Computer Science<br></p>
                          </div>
                          <div class="person">
                            <img src="students/zhenghuihuang.jpg" />
                            <p>ÈªÉÊ≠£Ëºù<br>NTU CSIE<br>with <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a></p>
                          </div>
                          <div class="person">
                            <img src="students/junweiduanmu.jpg" />
                            <p><a href="http://www.linkedin.com/in/ray-tuan-mu-a46257246">Á´ØÊú®Á´£ÂÅâ</a><br>Institute of<br>Computer Science<br></p>
                          </div>
                          <div class="person">
                            <img src="students/chengyoulu.jpg" />
                            <p><a href="https://johnnylu305.github.io/">ÂëÇÊâøÁ•ê</a><br>UTS CS<br>with <a href="https://profiles.uts.edu.au/Chin-Teng.Lin">Chin-Teng Lin</a></p>
                          </div>
                        </div>
                        <p><center><subheading>Research Assistants</subheading></center></p>
                        <div class="people">
                          <div class="person">
                            <img src="students/changhanyeh.jpg" />
                            <p><a href="https://jimmycv07.github.io/">ËëâÈï∑ÁÄö</a><br>BS, NYCU CE & CS<br>now MS student @ UIUC</p>
                          </div>
                          <div class="person">
                            <img src="students/tinghsuanchen.jpg" />
                            <p><a href="https://koi953215.github.io/">Èô≥ÈúÜËªí</a><br>BS, NCHU AMATH<br>now MS student @ USC</p>
                          </div>
                          <div class="person">
                            <img src="students/haojenchien.jpg" />
                            <p>Á∞°Êµ©‰ªª<br>MS, UCLA ECE<br></p>
                          </div>
                          <div class="person">
                            <img src="students/yuankanglee.jpg" />
                            <p><a href="https://ntuneillee.github.io">ÊùéÊ≤ÖÁΩ°</a><br>MS, NTU GICE</p>
                          </div>
                        </div>
                        <p><center><subheading>MS Students</subheading></center></p>
                        <div class="people">
                          <div class="person">
                            <img src="students/chinyanglin.jpg" />
                            <p><a href="https://linjohnss.github.io/">ÊûóÊôâÊöò</a><br>Institute of Data Science<br>with <a href="https://walonchiu.github.io/">Wei-Chen Chiu</a></p>
                          </div>
                          <div class="person">
                            <img src="students/kennethwu.jpg" />
                            <p><a href="https://kkennethwu.github.io/">Âê≥‰∏≠Ëµ´</a><br>Institute of<br>Multimedia<br></p>
                          </div>
                          <div class="person">
                            <img src="students/alexyen.jpg" />
                            <p>Âö¥Â£´ÂáΩ<br>Institute of<br>Multimedia<br></p>
                          </div>
                          <div class="person">
                            <img src="students/jiewenchan.jpg" />
                            <p>Èô≥Êç∑Êñá<br>Institute of<br>Computer Science<br></p>
                          </div>
                          <div class="person">
                            <img src="students/xuhaoxiang.jpg" />
                            <p>Ë®±ÁöìÁøî<br>Institute of Computer Science<br>with <a href="http://gpl.cs.nctu.edu.tw/Steve-Lin/">Wen-Chieh Lin</a></p>
                          </div>
                          <div class="person">
                            <img src="students/chengdefan.jpg" />
                            <p>ËåÉ‰∏ûÂæ∑<br>Institute of Computer Science<br>with <a href="https://sites.google.com/view/yctseng">Yu-Chee Tseng</a></p>
                          </div>
                          <div class="person">
                            <img src="students/chenweichang.jpg" />
                            <p>ÂºµÂÆ∏Áëã<br>Institute of AI Innovation<br>with <a href="https://people.cs.nycu.edu.tw/~jlhuang/">Jiun-Long Huang</a></p>
                          </div>
                          <div class="person">
                            <img src="students/yotinlin.jpg" />
                            <p>Êûó‰ΩëÂ∫≠<br>Institute of Computer Science<br>with <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a></p>
                          </div>
                          <div class="person">
                            <img src="students/tingweihuang.jpg" />
                            <p>ÈªÉ‰∫≠ÂπÉ<br>Institute of Computer Science<br>with <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a></p>
                          </div>
                          <div class="person">
                            <img src="students/yinghuanchen.jpg" />
                            <p>Èô≥Êò†ÂØ∞<br>Institute of<br>Computer Science<br></p>
                          </div>
                          <div class="person">
                            <img src="students/sianglingzhang.jpg" />
                            <p><a href="https://www.linkedin.com/in/sianglinzhang/">ÂºµÊ¨ÄÈΩ°</a><br>Institute of<br>Computer Science<br></p>
                          </div>
                          <div class="person">
                            <img src="students/huaish.jpg" />
                            <p><a href="https://www.linkedin.com/in/huaish/">ÈÑ≠Ê∑ÆËñ∞</a><br>Institute of<br>Computer Science<br></p>
                          </div>
                          <div class="person">
                            <img src="students/hentcike.jpg" />
                            <p><a href="https://hentci.github.io/">ÊüØÊüèÊó≠</a><br>Institute of Computer Science<br>with <a href="https://walonchiu.github.io/">Wei-Chen Chiu</a></p>
                          </div>
                          <div class="person">
                            <img src="students/pingchenwu.jpg" />
                            <p>Âê≥ÁßâÂÆ∏<br>Program of<br>Artificial Intelligence<br></p>
                          </div>
                          <div class="person">
                            <img src="students/haopingwang.jpg" />
                            <p>ÁéãÁöìÂπ≥<br>Institute of<br>Multimedia<br></p>
                          </div>
                          <div class="person">
                            <img src="students/weilingchi.jpg" />
                            <p>ÊàöÁ∂≠Âáå<br>Institute of<br>Multimedia<br></p>
                          </div>
                          <div class="person">
                            <img src="students/bofanyu.jpg" />
                            <p>‰øûÊüèÂ∏Ü<br>Institute of<br>Computer Science<br></p>
                          </div>
                          <div class="person">
                            <img src="students/jingenhuang.jpg" />
                            <p><a href="https://angusbb.github.io/">ÈªÉÈùñÊÅ©</a><br>Institute of Data Science<br>with <a href="https://homepage.citi.sinica.edu.tw/pages/pullpull/index_en.html">Jun-Cheng Chen</a></p>
                          </div>
                        </div>
                        <p><center><subheading>BS Students</subheading></center></p>
                        <div class="people">
                          <div class="person">
                            <img src="students/chihhaisu.jpg" />
                            <p><a href="https://su-terry.github.io/">ËòáÊô∫Êµ∑</a><br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/chihyaohu.jpg" />
                            <p>ËÉ°Êô∫Â†Ø<br>NTU MED<br></p>
                          </div>
                          <div class="person">
                            <img src="students/edisonlee55.webp" />
                            <p><a href="https://www.edisonlee55.com">ÊùéÊòéË¨ô</a><br>NYCU ARETEHP<br></p>
                          </div>
                          <div class="person">
                            <img src="images/avatar_scholar_256.png" />
                            <p>Â≠´ÊèöÂñÜ<br>NYCU MED<br></p>
                          </div>
                          <div class="person">
                            <img src="images/avatar_scholar_256.png" />
                            <p>Èô≥Â£´Âºò<br>NYCU MATH & CS<br>now MS student @ NYCU</p>
                          </div>
                          <div class="person">
                            <img src="images/avatar_scholar_256.png" />
                            <p>Èô≥Êò±‰Ωë<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/tsungyenlee.jpg" />
                            <p><a href="http://solocat17.github.io/">ÊùéÂÆóË´∫</a><br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="images/avatar_scholar_256.png" />
                            <p>Ê•äÂÆóÂÑí<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/kaihsinchen.jpg" />
                            <p>Èô≥Âá±Êòï<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/yirueiliu.jpg" />
                            <p>ÂäâÁèÜÁùø<br>NYCU CS<br>now MS student @ UIUC</p>
                          </div>
                          <div class="person">
                            <img src="images/avatar_scholar_256.png" />
                            <p>Âê≥‰øäÂÆè<br>NYCU CS<br>now MS student @ NYCU</p>
                          </div>
                          <div class="person">
                            <img src="students/shrrueitsai.jpg" />
                            <p>Ëî°Â∏´Áùø<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/WeiChengChang.jpg" />
                            <p>ÂºµÁ∂≠Á®ã<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/jylee.jpg" />
                            <p><a href="https://jayinnn.dev/">ÊùéÊù∞Á©é</a><br>NYCU CS<br>now exchange student @ ETHZ</p>
                          </div>
                          <div class="person">
                            <img src="students/0816109.jpg" />
                            <p><a href="https://www.linkedin.com/in/ericyangchen">Èô≥Ê•äËûç</a><br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="images/avatar_scholar_256.png" />
                            <p>Âê≥ÂÆöÈúñ<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/lizhongsitu.jpg" />
                            <p>Âè∏ÂæíÁ´ã‰∏≠<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/yitingchu.jpg" />
                            <p>Êú±È©õÂ∫≠<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/hehsu.jpg" />
                            <p>ÂæêÂíå<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="images/avatar_scholar_256.png" />
                            <p>ÁøÅÊô®Êò±<br>NTHU PHYS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/yihsiangho.jpg" />
                            <p><a href="https://sean20405.github.io/">‰ΩïÁæ©Áøî</a><br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/yuchehsieh.jpg" />
                            <p>Ë¨ù‰æëÂì≤<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/yuweitsai.jpg" />
                            <p>Ëî°ËÅøÁëã<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="students/minghsiangcheng.jpg" />
                            <p>ÈÑ≠ÂêçÁøî<br>NYCU ARETEHP<br></p>
                          </div>
                          <div class="person">
                            <img src="students/chiachenyeh.jpg" />
                            <p>ËëâÂÆ∂ËìÅ<br>NYCU CS<br></p>
                          </div>
                          <div class="person">
                            <img src="images/avatar_scholar_256.png" />
                            <p>ÊûóÊèöÊ£Æ<br>NYCU ME & CS<br></p>
                          </div>
                          <div class="person">
                            <img src="images/avatar_scholar_256.png" />
                            <p>Ëî°ÊòÄÈåö<br>NYCU CS<br>with <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a><br></p>
                          </div>
                          <div class="person">
                            <img src="students/chiehhsing.jpg" />
                            <p>ÈÇ¢Êç∑<br>Shanghai University<br>MATH<br></p>
                          </div>
                        </div>
                        <p><center><a href="javascript:toggleblock('alumni')"><subheading>Alumni</subheading></a></center></p>
                        <!-- <subsectionheading>
                          <a href="javascript:toggleblock('alumni')">Alumni</a>
                        </subsectionheading> -->
                        <i id="alumni">
                          <div class="people">
                            <div class="person">
                              <img src="students/boyucheng.jpg" />
                              <p>ÈÑ≠‰ºØ‰øû<br>MS Spring 2023<br>with <a href="https://walonchiu.github.io/">Wei-Chen Chiu</a><br>now @ Qualcomm</p>
                            </div>
                            <div class="person">
                              <img src="images/avatar_scholar_256.png" />
                              <p>Èô≥‰øäÁëã<br>BS 2023<br>Next MS Student<br>NYCU</p>
                            </div>
                            <div class="person">
                              <img src="images/avatar_scholar_256.png" />
                              <p>ÊûóÂ•ïÊù∞<br>BS 2023-2024<br></p>
                            </div>
                            <div class="person">
                              <img src="images/avatar_scholar_256.png" />
                              <p>ÁæÖÂÆáÂëà<br>BS 2023-2024<br></p>
                            </div>
                            <div class="person">
                              <img src="images/avatar_scholar_256.png" />
                              <p>ÈÉ≠Áé†Áî´<br>BS 2023-2024<br></p>
                            </div>
                            <div class="person">
                              <img src="images/avatar_scholar_256.png" />
                              <p>ËëâÊüîÊòÄ<br>BS 2023-2024<br></p>
                            </div>
                            <div class="person">
                              <img src="students/CHENG YU-HAO.jpg" />
                              <p>ÈÑ≠ÂèàË±™<br>BS 2023-2024<br></p>
                            </div>
                            <div class="person">
                              <img src="images/avatar_scholar_256.png" />
                              <p>‰∏ÅÁ•êÊâø<br>BS 2023-2024<br></p>
                            </div>
                            <div class="person">
                              <img src="students/kevinhsieh1016.jpg" />
                              <p>Ë¨ùÊòéÁø∞<br>BS 2023-2024<br></p>
                            </div>
                            <div class="person">
                              <img src="students/weizhishi.jpg" />
                              <p>ÊñΩÊÉüÊô∫<br>BS 2023-2024<br></p>
                            </div>
                            <div class="person">
                              <img src="students/erniechu.jpg" />
                              <p><a href="https://ernestchu.github.io/">Êú±Âä≠Áíø</a><br>MS Spring 2024<br>now PhD Student<br>@ Johns Hopkins</p>
                            </div>
                          </div>
                        </i>
                      </td>
                    </tr>
                  </tbody></table>
                  
                  
                  <!-- <li>ÊûóÊôâÊöò, 2023</li>
                          <li>Âê≥‰∏≠Ëµ´, 2023</li>
                          <li>Âö¥Â£´ÂáΩ, 2023</li>
                          <li>Èô≥Êç∑Êñá, 2023</li>
                          <li>ÈÑ≠‰ºØ‰øû, 2023 (with <a href="https://walonchiu.github.io/">Wei-Chen Chiu</a>)</li>
                        </ul>
                      </td>
                      <td style="padding:20px;width:33%;vertical-align:top">
                        <subheading>Undergraduate Students</subheading>
                        <br>
                        <ul style="list-style:none; padding-left:0;">
                          <li>ËòáÊô∫Êµ∑, 2023</li>
                          <li>ËÉ°Êô∫Â†Ø, 2023</li>
                          <li>Èô≥‰øäÁëã, 2023</li>
                          <li>ÊùéÊòéË¨ô, 2023</li>
                          <li>ÊûóÂ•ïÊù∞, 2023</li>
                          <li>Â≠´ÊèöÂñÜ, 2023</li>
                          <li>ÁæÖÂÆáÂëà, 2023</li>
                          <li>ÈÉ≠Áé†Áî´, 2023</li>
                          <li>ËëâÊüîÊòÄ, 2023</li>
                          <li>Èô≥Â£´Âºò, 2023</li>
                          <li>ÈÑ≠ÂèàË±™, 2023</li>
                          <li>Èô≥Êò±‰Ωë, 2023</li>
                          <li>‰∏ÅÁ•êÊâø, 2023</li>
                          <li>ÊùéÂÆóË´∫, 2023</li>
                          <li>Ê•äÂÆóÂÑí, 2023</li>
                          <li>Èô≥Âá±Êòï, 2023</li>
                          <li>ÂäâÁèÜÁùø, 2023</li>
                          <li>Âê≥‰øäÂÆè, 2023</li>
                          <li>Ëî°Â∏´Áùø, 2023</li>
                          <li>ÂºµÁ∂≠Á®ã, 2023</li> -->
                  
                  
                  <hr class="soft">
                  
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <center><heading>Research</heading></center>
                      </td>
                    </tr>
                  </tbody></table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                    <div class="text-center">
                      <ul class="nav nav-pills center-pills">
                          <li class="method-pill" data-value="All">
                              <a><i class="pe-2 bi bi-list"></i> All Papers</a>
                          </li>
                          <li class="method-pill active" data-value="Representative">
                              <a><i class="pe-2 bi bi-stars"></i> Representative</a>
                          </li>
                          <li class="line-break"></li>
                          <li class="method-pill" data-value="Image and Video Synthesis and Generation">
                            <!-- 14 -->
                              <a>Image and Video Synthesis and Generation</a>
                          </li>
                          <li class="method-pill" data-value="3D from Multi-view and Sensors">
                            <!-- 13 -->
                              <a>3D from Multi-view and Sensors</a>
                          </li>
                          <li class="method-pill" data-value="Low-level Vision">
                            <!-- 13 -->
                              <a>Low-level Vision</a>
                          </li>
                          <li class="method-pill" data-value="Video: Low-level Analysis, Motion, and Tracking">
                            <!-- 6 -->
                              <a>Video: Low-level Analysis, Motion, and Tracking</a>
                          </li>
                          <li class="method-pill" data-value="Computational Imaging">
                            <!-- 4 -->
                              <a>Computational Imaging</a>
                          </li>
                          <li class="method-pill" data-value="Vision + Graphics">
                            <!-- 3 -->
                              <a>Vision + Graphics</a>
                          </li>
                          <li class="method-pill" data-value="Segmentation, Grouping and Shape Analysis">
                            <!-- 3 -->
                              <a>Segmentation, Grouping and Shape Analysis</a>
                          </li>
                          <li class="method-pill" data-value="Humans: Face, Body, Pose, Gesture, Movement">
                            <!-- 3 -->
                              <a>Humans: Face, Body, Pose, Gesture, Movement</a>
                          </li>
                          <li class="method-pill" data-value="3D from Single Images">
                            <!-- 2 -->
                              <a>3D from Single Images</a>
                          </li>
                          <li class="method-pill" data-value="Medical and Biological Vision, Cell Microscopy">
                            <!-- 1 -->
                              <a>Medical and Biological Vision, Cell Microscopy</a>
                          </li>
                          <li class="method-pill" data-value="Robotics">
                            <!-- 1 -->
                              <a>Robotics</a>
                          </li>
                          <li class="method-pill" data-value="Embodied Vision: Active Agents, Simulation">
                            <!-- 1 -->
                              <a>Embodied Vision: Active Agents, Simulation</a>
                          </li>
                          <li class="method-pill" data-value="Physics-based Vision and Shape-from-X">
                            <!-- 1 -->
                              <a>Physics-based Vision and Shape-from-X</a>
                          </li>
                          <li class="method-pill" data-value="Autonomous Driving">
                            <!-- 1 -->
                              <a>Autonomous Driving</a>
                          </li>
                          <li class="method-pill" data-value="Vision, Language, and Reasoning">
                            <!-- 1 -->
                              <a>Vision, Language, and Reasoning</a>
                          </li>
                      </ul>
                  </div>
                  
                  
                    
                    <!-- <div class="row">
                        <div class="col-lg-3 col-12 order-first order-lg-last pt-4 pt-lg-0 d-none d-lg-block">
                            <nav class="nav flex-lg-column nav-pills ms-lg-4 mb-lg-4">
                                <button class="nav-link text-start">
                                    <i class="pe-2 bi bi-list"></i>
                                    <span>All</span>
                                    Papers
                                </button>
                                <button class="nav-link text-start active">
                                    <i class="pe-2 bi bi-stars"></i>
                                    <span>Representative</span>
                                </button>
                                <button class="nav-link text-start"><span>Low-level Vision</span></button>
                                <button class="nav-link text-start"><span>3D from Single Images</span></button>
                                <button class="nav-link text-start"><span>Image and Video Synthesis and Generation</span></button>
                                <button class="nav-link text-start"><span>Medical and Biological Vision, Cell Microscopy</span></button>
                                <button class="nav-link text-start"><span>Robotics</span></button>
                                <button class="nav-link text-start"><span>3D from Multi-view and Sensors</span></button>
                                <button class="nav-link text-start"><span>Embodied Vision: Active Agents, Simulation</span></button>
                                <button class="nav-link text-start"><span>Segmentation, Grouping and Shape Analysis</span></button>
                                <button class="nav-link text-start"><span>Humans: Face, Body, Pose, Gesture, Movement</span></button>
                                <button class="nav-link text-start"><span>Physics-based Vision and Shape-from-X</span></button>
                                <button class="nav-link text-start"><span>Video: Low-level Analysis, Motion, and Tracking</span></button>
                                <button class="nav-link text-start"><span>Computational Imaging</span></button>
                            </nav>
                        </div> -->



                        <!-- <div class="col-lg-12">
                          <ul class="nav nav-pills justify-content-center" role="navigation">
                              <li class="nav-item">
                                  <a class="nav-link disabled" href="#" tabindex="-1" aria-disabled="true" style="width: 110px"><strong>Research:</strong></a>
                              </li>
                              <li class="nav-item method-pill" data-value="all">
                                  <a class="nav-link">All Papers</a>
                              </li>
                              <li class="nav-item method-pill" data-value="representative">
                                  <a class="nav-link active">Representative</a>
                              </li>
                              <li class="nav-item method-pill" data-value="low-level-vision">
                                  <a class="nav-link">Low-level Vision</a>
                              </li>
                              <li class="nav-item method-pill" data-value="3d-single-images">
                                  <a class="nav-link">3D from Single Images</a>
                              </li>
                              <li class="nav-item method-pill" data-value="image-video-synthesis">
                                  <a class="nav-link">Image and Video Synthesis and Generation</a>
                              </li>
                              <li class="nav-item method-pill" data-value="medical-biological">
                                  <a class="nav-link">Medical and Biological Vision, Cell Microscopy</a>
                              </li>
                              <li class="nav-item method-pill" data-value="robotics">
                                  <a class="nav-link">Robotics</a>
                              </li>
                              <li class="nav-item method-pill" data-value="3d-multi-view">
                                  <a class="nav-link">3D from Multi-view and Sensors</a>
                              </li>
                              <li class="nav-item method-pill" data-value="embodied-vision">
                                  <a class="nav-link">Embodied Vision: Active Agents, Simulation</a>
                              </li>
                              <li class="nav-item method-pill" data-value="segmentation">
                                  <a class="nav-link">Segmentation, Grouping and Shape Analysis</a>
                              </li>
                              <li class="nav-item method-pill" data-value="humans">
                                  <a class="nav-link">Humans: Face, Body, Pose, Gesture, Movement</a>
                              </li>
                          </ul>
                      </div> -->
                        
                        <div class="col-lg-9 col-12 px-4 papers-container">
                            <!-- <div class="year"><p><center><subsubheading>2024</subsubheading></center></p></div> -->

                            <p><center><subheading id="paper-heading">Representative Papers</subheading></center></p>

                            <!-- Year Row -->
                            <tr class="year-group">
                              <td colspan="2" class="year">
                                <br><center><subsubheading>2024</subsubheading></center>
                              </td>
                            </tr>

                            <tr bgcolor="#ffffff" class="publication paper-Low-level-Vision paper-Low-level-Vision">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two"><img src='images/Sun2024FIP.jpg' width="160" alt="Description" style="margin-top: 40px;"></div>
                                  <img src='images/Sun2024FIP.jpg' width="160" alt="Description" style="margin-top: 40px;">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://jayisaking.github.io/FIPER/">
                                  <papertitle>FIPER: Generalizable Factorized Fields for Joint Image Compression and Super-Resolution</papertitle>
                                </a>
                                <br>
                                Yang-Che Sun, Cheng Yu Yeo, 
                                <a href="https://www.cs.jhu.edu/~schu23/">Ernie Chu</a>, 
                                <a href="https://homepage.citi.sinica.edu.tw/pages/pullpull/index_en.html">Jun-Cheng Chen</a>, 
                                <strong>Yu-Lun Liu</strong>
                                <br>
                                <em>arXiv</em>, 2024 &nbsp
                                <img src="images/new_animated.gif"\>
                                <br>
                                <a href="https://jayisaking.github.io/FIPER/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2410.18083">arXiv</a>
                                /
                                <a href="">code [coming soon]</a>
                                <p></p>
                                <p>
                                  This work derives a SR model, which includes a Coefficient Backbone and Basis Swin Transformer for generalizable Factorized Fields, and introduces a merged-basis compression branch that consolidates shared structures, further optimizing the compression process.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Fan2024SMD_stop()" onmouseover="Fan2024SMD_start()"  bgcolor="#ffffff" class="publication paper-3D-from-Multi-view-and-Sensors paper-3D-from-Multi-view-and-Sensors">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Fan2024SMD_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Fan2024SMD.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                  <img src='images/Fan2024SMD.jpg' width="160" alt="Description" style="margin-top: 35px;">
                                </div>
                                <script type="text/javascript">
                                  function Fan2024SMD_start() {
                                    document.getElementById('Fan2024SMD_image').style.opacity = "1";
                                  }
                  
                                  function Fan2024SMD_stop() {
                                    document.getElementById('Fan2024SMD_image').style.opacity = "0";
                                  }
                                  Fan2024SMD_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://cdfan0627.github.io/spectromotion/">
                                  <papertitle>SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes</papertitle>
                                </a>
                                <br>
                                Cheng-De Fan, <a href="https://chenwei891213.github.io/">Chen-Wei Chang</a>, Yi-Ruei Liu , <a href="https://jayinnn.dev/">Jie-Ying Lee</a>, <a href="https://people.cs.nycu.edu.tw/~jlhuang/">Jiun-Long Huang</a>, <a href="https://sites.google.com/view/yctseng">Yu-Chee Tseng</a>, <strong>Yu-Lun Liu</strong>
                                <br>
                                <em>arXiv</em>, 2024
                                <img src="images/new_animated.gif"\>
                                <br>
                                <a href="https://cdfan0627.github.io/spectromotion/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2410.17249">arXiv</a>
                                /
                                <a href="https://drive.google.com/file/d/1ciuPR-1RMo0-5Bn6wz2qMliFywxGjRpu/view?usp=sharing">results</a>
                                /
                                <a href="">code [comming soon]</a>
                                <p></p>
                                <p>
                                  SpectroMotion is presented, a novel approach that combines 3D Gaussian Splatting with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes and is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Lin2024FNR_stop()" onmouseover="Lin2024FNR_start()"  bgcolor="#ffffd0" class="publication paper-3D-from-Multi-view-and-Sensors paper-3D-from-Multi-view-and-Sensors paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Lin2024FNR_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Lin2024FNR.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                  <img src='images/Lin2024FNR.jpg' width="160" alt="Description" style="margin-top: 0px;">
                                </div>
                                <script type="text/javascript">
                                  function Lin2024FNR_start() {
                                    document.getElementById('Lin2024FNR_image').style.opacity = "1";
                                  }
                  
                                  function Lin2024FNR_stop() {
                                    document.getElementById('Lin2024FNR_image').style.opacity = "0";
                                  }
                                  Lin2024FNR_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://linjohnss.github.io/frugalnerf/">
                                  <papertitle>FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors</papertitle>
                                </a>
                                <br>
                                <a href="https://linjohnss.github.io/">Chin-Yang Lin</a>, <a href="https://kkennethwu.github.io/">Chung-Ho Wu</a>, <a href="https://jimmycv07.github.io/">Chang-Han Yeh</a>, Shih-Han Yen, <a href="https://sunset1995.github.io/">Cheng Sun</a>, <strong>Yu-Lun Liu</strong>
                                <br>
                                <em>arXiv</em>, 2024
                                <img src="images/new_animated.gif"\>
                                <br>
                                <a href="https://linjohnss.github.io/frugalnerf/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2410.16271">arXiv</a>
                                /
                                <a href="https://linjohnss.github.io/">code [comming soon]</a>
                                <p></p>
                                <p>
                                  FrugalNeRF is introduced, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details and guides training without relying on externally learned priors, enabling full utilization of the training data.
                              </td>
                            </tr>
                            
                            <tr onmouseout="Yeh2024DIR_stop()" onmouseover="Yeh2024DIR_start()"  bgcolor="#ffffff" class="publication paper-Low-level-Vision paper-Low-level-Vision">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Yeh2024DIR_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Yeh2024DIR.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                  <img src='images/Yeh2024DIR.jpg' width="160" alt="Description" style="margin-top: 35px;">
                                </div>
                                <script type="text/javascript">
                                  function Yeh2024DIR_start() {
                                    document.getElementById('Yeh2024DIR_image').style.opacity = "1";
                                  }
                  
                                  function Yeh2024DIR_stop() {
                                    document.getElementById('Yeh2024DIR_image').style.opacity = "0";
                                  }
                                  Yeh2024DIR_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://jimmycv07.github.io/DiffIR2VR_web/">
                                  <papertitle>DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models</papertitle>
                                </a>
                                <br>
                                <a href="https://jimmycv07.github.io/">Chang-Han Yeh</a>, <a href="https://linjohnss.github.io/">Chin-Yang Lin</a>, <a href="https://lightchaserx.github.io/">Zhixiang Wang</a>, <a href="https://chiweihsiao.github.io/">Chi-Wei Hsiao</a>, <a href="https://koi953215.github.io/">Ting-Hsuan Chen</a>, <strong>Yu-Lun Liu</strong>
                                <br>
                                <em>arXiv</em>, 2024
                                <img src="images/new_animated.gif"\>
                                <br>
                                <a href="https://jimmycv07.github.io/DiffIR2VR_web/">project page</a>
                                /
                                <a href="http://arxiv.org/abs/2407.01519">arXiv</a>
                                /
                                <a href="https://github.com/jimmycv07/DiffIR2VR-Zero">code</a>
                                /
                                <a href="https://huggingface.co/spaces/Koi953215/DiffIR2VR">demo</a>
                                <p></p>
                                <p>
                                  It is shown that this method not only achieves top performance in zero-shot video restoration but also significantly surpasses trained models in generalization across diverse datasets and extreme degradations.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Wu2024DNV_stop()" onmouseover="Wu2024DNV_start()"  bgcolor="#ffffff" class="publication paper-Medical-and-Biological-Vision-Cell-Microscopy paper-Medical-and-Biological-Vision-Cell-Microscopy">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Wu2024DNV_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Wu2024DNV.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                  <img src='images/Wu2024DNV.jpg' width="160" alt="Description" style="margin-top: 0px;">
                                </div>
                                <script type="text/javascript">
                                  function Wu2024DNV_start() {
                                    document.getElementById('Wu2024DNV_image').style.opacity = "1";
                                  }
                  
                                  function Wu2024DNV_stop() {
                                    document.getElementById('Wu2024DNV_image').style.opacity = "0";
                                  }
                                  Wu2024DNV_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://kirito878.github.io/DeNVeR/">
                                  <papertitle>DeNVeR: Deformable Neural Vessel Representations for Unsupervised Video Vessel Segmentation</papertitle>
                                </a>
                                <br>
                                Chun-Hung Wu, Shih-Hong Chen, Chih-Yao Hu, Hsin-Yu Wu, Kai-Hsin Chen, Yu-You Chen, <a href="https://su-terry.github.io/">Chih-Hai Su</a>, Chih-Kuo Lee, <strong>Yu-Lun Liu</strong>
                                <br>
                                <em>arXiv</em>, 2024
                                <img src="images/new_animated.gif"\>
                                <br>
                                <a href="https://kirito878.github.io/DeNVeR/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2406.01591">arXiv</a>
                                /
                                <a href="">code & dataset [coming soon]</a>
                                /
                                <a href="https://colab.research.google.com/drive/1IYGiJECwAaoLPq7KGHQE_dvtrdHz9fUA?authuser=2&hl=zh-tw#scrollTo=n1ppvOhqbRkV">colab</a>
                                <p></p>
                                <p>
                                  DeNVeR is presented, an unsupervised approach for vessel segmentation in X-ray videos without annotated ground truth, providing a robust, data-efficient tool for disease diagnosis and treatment planning and setting a new standard for future research in video vessel segmentation.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Hsiao2024RFL_stop()" onmouseover="Hsiao2024RFL_start()"  bgcolor="#ffffd0" class="publication paper-Low-level-Vision paper-Image-and-Video-Synthesis-and-Generation">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Hsiao2024RFL_image'>
                                    <img src='images/Hsiao2024RFL_output.jpg' width="160" style="margin-top: 10px;"></div>
                                  <img src='images/Hsiao2024RFL_input.jpg' width="160" alt="Description" style="margin-top: 10px;">
                                </div>
                                <script type="text/javascript">
                                  function Hsiao2024RFL_start() {
                                    document.getElementById('Hsiao2024RFL_image').style.opacity = "1";
                                  }
                  
                                  function Hsiao2024RFL_stop() {
                                    document.getElementById('Hsiao2024RFL_image').style.opacity = "0";
                                  }
                                  Hsiao2024RFL_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://chiweihsiao.github.io/refldm.github.io/">
                                  <papertitle>ReF-LDM: A Latent Diffusion Model for Reference-based Face Image Restoration</papertitle>
                                </a>
                                <br>
                                <a href="https://www.linkedin.com/in/chiwei-hsiao">Chi-Wei Hsiao</a>, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="https://tw.linkedin.com/in/cheng-kun-yang-497950119">Cheng-Kun Yang</a>, 
                                Sheng-Po Kuo, Yucheun Kevin Jou, 
                                <a href="https://tw.linkedin.com/in/chia-ping-chen-81674078">Chia-Ping Chen</a>
                                <br>
                                <em>NeurIPS</em>, 2024
                                <br>
                                <a href="https://chiweihsiao.github.io/refldm.github.io/">project page</a>
                                /
                                <a href="https://chiweihsiao.github.io/refldm.github.io/static/NeurIPS_2024_ReF-LDM.pdf">paper</a>
                                /
                                <a href="https://github.com/ChiWeiHsiao/ref-ldm">code</a>
                                <p></p>
                                <p>
                                  This work proposes ReF-LDM, an adaptation of LDM designed to generate HQ face images conditioned on one LQ image and multiple HQ reference images, and designs a timestep-scaled identity loss, enabling the LDM-based model to focus on learning the discriminating features of human faces.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Wang2024DAE_stop()" onmouseover="Wang2024DAE_start()"  bgcolor="#ffffd0" class="publication paper-3D-from-Single-Images paper-3D-from-Single-Images paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Wang2024DAE_image'>
                                    <img src='images/Wang2024DAE_output.jpg' width="160" style="margin-top: 35px;"></div>
                                  <img src='images/Wang2024DAE_input.jpg' width="160" alt="Description" style="margin-top: 35px;">
                                </div>
                                <script type="text/javascript">
                                  function Wang2024DAE_start() {
                                    document.getElementById('Wang2024DAE_image').style.opacity = "1";
                                  }
                  
                                  function Wang2024DAE_stop() {
                                    document.getElementById('Wang2024DAE_image').style.opacity = "0";
                                  }
                                  Wang2024DAE_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://albert100121.github.io/Depth-Anywhere/">
                                  <papertitle>Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation</papertitle>
                                </a>
                                <br>
                                <a href="http://albert100121.github.io/">Ning-Hsu Wang</a>, <strong>Yu-Lun Liu</strong>
                                <br>
                                <em>NeurIPS</em>, 2024
                                <br>
                                <a href="https://albert100121.github.io/Depth-Anywhere/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2406.12849">arXiv</a>
                                /
                                <a href="">code [coming soon]</a>
                                /
                                <a href="https://huggingface.co/spaces/Albert-NHWang/Depth-Anywhere-App">demo</a>
                                <p></p>
                                <p>
                                  This work proposes a new depth estimation framework that utilizes unlabeled 360-degree data effectively and uses state-of-the-art perspective depth estimation models as teacher models to generate pseudo labels through a six-face cube projection technique, enabling efficient labeling of depth in 360-degree images.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Chen2024NRC_stop()" onmouseover="Chen2024NRC_start()"  bgcolor="#ffffd0" class="publication paper-Image-and-Video-Synthesis-and-Generation paper-Image-and-Video-Synthesis-and-Generation paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Chen2024NRC_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Chen2024NRC.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                  <img src='images/Chen2024NRC.jpg' width="160" alt="Description" style="margin-top: 35px;">
                                </div>
                                <script type="text/javascript">
                                  function Chen2024NRC_start() {
                                    document.getElementById('Chen2024NRC_image').style.opacity = "1";
                                  }
                  
                                  function Chen2024NRC_stop() {
                                    document.getElementById('Chen2024NRC_image').style.opacity = "0";
                                  }
                                  Chen2024NRC_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://koi953215.github.io/NaRCan_page/">
                                  <papertitle>NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing</papertitle>
                                </a>
                                <br>
                                <a href="https://koi953215.github.io/">Ting-Hsuan Chen</a>, Jiewen Chan, Hau-Shiang Shiu, Shih-Han Yen, <a href="https://jimmycv07.github.io/">Chang-Han Yeh</a>, <strong>Yu-Lun Liu</strong>
                                <br>
                                <em>NeurIPS</em>, 2024
                                <br>
                                <a href="https://koi953215.github.io/NaRCan_page/">project page</a>
                                /
                                <a href="http://arxiv.org/abs/2406.06523">arXiv</a>
                                /
                                <a href="https://drive.google.com/file/d/1Ur6HEP2MizFe3z7iSXoaMQmC5_qHUn1S/view?usp=sharing">results</a>
                                /
                                <a href="https://github.com/koi953215/NaRCan">code</a>
                                /
                                <a href="https://youtu.be/ufYRGldCV78">video</a>
                                /
                                <a href="https://huggingface.co/spaces/Koi953215/NaRCan_demo">demo</a>
                                <p></p>
                                <p>
                                  A video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model's ability to handle complex video dynamics.
                              </td>
                            </tr>

                            <tr onmouseout="Guo2024PPPstop()" onmouseover="Guo2024PPP_start()"  bgcolor="#ffffff" class="publication paper-Robotics paper-Robotics">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Guo2024PPP_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Guo2024PPP.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                    <img src='images/Guo2024PPP.jpg' width="160" alt="Description" style="margin-top: 35px;">
                                </div>
                                <script type="text/javascript">
                                  function Guo2024PPP_start() {
                                    document.getElementById('Guo2024PPP_image').style.opacity = "1";
                                  }
                  
                                  function Guo2024PPPstop() {
                                    document.getElementById('Guo2024PPP_image').style.opacity = "0";
                                  }
                                  Guo2024PPPstop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://tony2guo.github.io/precise-pick-and-place/">
                                  <papertitle>Precise Pick-and-Place using Score-Based Diffusion Networks</papertitle>
                                </a>
                                <br>
                                <a href="https://tony2guo.github.io/">Shih-Wei Guo</a>, 
                                <a href="https://github.com/Ending2015a">Tsu-Ching Hsiao</a>, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="https://scholar.google.com/citations?hl=en&user=5mYNdo0AAAAJ&view_op=list_works&sortby=pubdate">Chun-Yi Lee</a>
                                <br>
                                <em>IROS</em>, 2024
                                <br>
                                <a href="https://tony2guo.github.io/precise-pick-and-place/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2409.09725">arXiv</a>
                                /
                                <a href="https://www.youtube.com/watch?v=56RQPgYyP1s">video</a>
                                <p></p>
                                <p>
                                  A novel coarse-to-fine continuous pose diffusion method to enhance the precision of pick-and-place operations within robotic manipulation tasks and facilitates the accurate perception of object poses, which enables more precise object manipulation.
                              </td>
                            </tr>
                  
                            <tr bgcolor="#ffffff" class="publication paper-3D-from-Multi-view-and-Sensors paper-3D-from-Multi-view-and-Sensors">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two"><img src='images/Li2024GRC.jpg' width="160" alt="Description" style="margin-top: 10px;"></div>
                                  <img src='images/Li2024GRC.jpg' width="160" alt="Description" style="margin-top: 10px;">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://minfenli.github.io/GenRC/">
                                  <papertitle>GenRC: 3D Indoor Scene Generation from Sparse Image Collections</papertitle>
                                </a>
                                <br>
                                <a href="https://minfenli.github.io/">Ming-Feng Li</a>, 
                                Yueh-Feng Ku, Hong-Xuan Yen, Chi Liu, 
                                <strong>Yu-Lun Liu</strong>, 
                                Albert Y. C. Chen, Cheng-Hao Kuo, 
                                <a href="https://aliensunmin.github.io/">Min Sun</a>
                                <br>
                                <em>ECCV</em>, 2024 &nbsp
                                <br>
                                <a href="https://minfenli.github.io/GenRC/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2407.12939">arXiv</a>
                                /
                                <a href="https://github.com/minfenli/GenRC">code</a>
                                <p></p>
                                <p>
                                  The proposed GenRC, an automated training-free pipeline to complete a room-scale 3D mesh with high-fidelity textures, outperforms state-of-the-art methods under most appearance and geometric metrics on ScanNet and ARKitScenes datasets, even though GenRC is not trained on these datasets nor using predefined camera trajectories.
                              </td>
                            </tr>
                  
                            <tr bgcolor="#ffffff" class="publication paper-Embodied-Vision-Active-Agents-Simulation paper-Autonomous-Driving">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <br>
                                  <br>
                                  <div class="two"><img src='images/icraw_myshen24.jpg' width="160"></div>
                                  <img src='images/icraw_myshen24.jpg' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://arxiv.org/abs/2403.15791">
                                  <papertitle>DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving Environment for Real-World Performance Validation</papertitle>
                                </a>
                                <br>
                                <a href="https://scholar.google.com.tw/citations?user=2bYyZnwAAAAJ&hl=zh-TW">Mu-Yi Shen</a>, 
                                Chia-Chi Hsu, Hao-Yu Hou, Yu-Chen Huang, 
                                <a href="https://scholar.google.com/citations?user=TgMlVRUAAAAJ&hl=zh-TW">Wei-Fang Sun</a>, 
                                <a href="https://scholar.google.com.tw/citations?user=FK1RcpoAAAAJ&hl=zh-TW">Chia-Che Chang</a>, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="https://elsalab.ai/about">Chun-Yi Lee</a>
                                <br>
                                <em>ICRA RoboNerF Workshop</em>, 2024 &nbsp
                                <br>
                                <a href="https://muyishen2040.github.io/DriveEnvNeRF/">project page</a>
                                /
                                <a href="https://youtu.be/8wNflV_A5FM">video</a>
                                /
                                <a href="https://arxiv.org/abs/2403.15791">arXiv</a>
                                /
                                <a href="https://github.com/muyishen2040/DriveEnvNeRF">code</a>
                                <p></p>
                                <p>
                                  The DriveEnv-NeRF framework, which leverages Neural Radiance Fields (NeRF) to enable the validation and faithful forecasting of the efficacy of autonomous driving agents in a targeted real-world scene, can serve as a training environment for autonomous driving agents under various lighting conditions.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Wang2024MBG_stop()" onmouseover="Wang2024MBG_start()"  bgcolor="#ffffff" class="publication paper-Segmentation-Grouping-and-Shape-Analysis paper-Image-and-Video-Synthesis-and-Generation">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Wang2024MBG_image'>
                                    <img src='images/Wang2024MBG_output.jpg' width="160" style="margin-top: 10px;"></div>
                                  <img src='images/Wang2024MBG_input.jpg' width="160" alt="Description" style="margin-top: 10px;">
                                </div>
                                <script type="text/javascript">
                                  function Wang2024MBG_start() {
                                    document.getElementById('Wang2024MBG_image').style.opacity = "1";
                                  }
                  
                                  function Wang2024MBG_stop() {
                                    document.getElementById('Wang2024MBG_image').style.opacity = "0";
                                  }
                                  Wang2024MBG_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://lightchaserx.github.io/matting-by-generation/">
                                  <papertitle>Matting by Generation</papertitle>
                                </a>
                                <br>
                                <a href="https://lightchaserx.github.io/">Zhixiang Wang</a>, 
                                Baiang Li, 
                                <a href="https://jianwang-cmu.github.io/">Jian Wang</a>, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="https://www.gujinwei.org/">Jinwei Gu</a>, 
                                <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
                                <a href="http://www.satoh-lab.nii.ac.jp/index.html">Shin'ichi Satoh</a>
                                <br>
                                <em>SIGGRAPH</em>, 2024 &nbsp
                                <br>
                                <a href="https://lightchaserx.github.io/matting-by-generation/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2407.21017">arXiv</a>
                                /
                                <a href="">code [coming soon]</a>
                                /
                                <a href="https://drive.google.com/file/d/1S6vpkuQs3I1pAX7FNRUy_ADtTudn7YJP/view">slides</a>
                                /
                                <a href="https://dl.acm.org/doi/suppl/10.1145/3641519.3657519/suppl_file/_SIGGRAPH24__Matting_by_Generation%20(10).pdf">supplement</a>
                                <p></p>
                                <p>
                                  An innovative approach for image matting that redefines the traditional regression-based task as a generative modeling challenge and harnesses the capabilities of latent diffusion models, enriched with extensive pre-trained knowledge, to regularize the matting process.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Su2024BMNstop()" onmouseover="Su2024BMN_start()"  bgcolor="#ffffd0" class="publication paper-3D-from-Multi-view-and-Sensors paper-Vision--Graphics paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Su2024BMN_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Su2024BMN.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                    <img src='images/Su2024BMN.jpg' width="160" alt="Description" style="margin-top: 20px;">
                                </div>
                                <script type="text/javascript">
                                  function Su2024BMN_start() {
                                    document.getElementById('Su2024BMN_image').style.opacity = "1";
                                  }
                  
                                  function Su2024BMNstop() {
                                    document.getElementById('Su2024BMN_image').style.opacity = "0";
                                  }
                                  Su2024BMNstop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://su-terry.github.io/BoostMVSNeRFs/">
                                  <papertitle>BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis in Large-scale Scenes</papertitle>
                                </a>
                                <br>
                                <a href="https://su-terry.github.io/">Chih-Hai Su*</a>, 
                                <a href="https://orcid.org/0009-0008-5968-6322">Chih-Yao Hu*</a>, 
                                <a href="https://orcid.org/0009-0002-2707-0095">Shr-Ruei Tsai*</a>, 
                                <a href="https://orcid.org/0009-0008-0826-4664">Jie-Ying Lee*</a>, 
                                <a href="https://linjohnss.github.io/">Chin-Yang Lin</a>, 
                                <strong>Yu-Lun Liu</strong>
                                <br>
                                <em>SIGGRAPH</em>, 2024 &nbsp
                                <br>
                                <a href="https://su-terry.github.io/BoostMVSNeRFs/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2407.15848">arXiv</a>
                                /
                                <a href="https://github.com/Su-Terry/BoostMVSNeRFs">code</a>
                                /
                                <a href="https://www.youtube.com/watch?v=tX4EkFgm0ng">video</a>
                                <p></p>
                                <p>
                                  This paper presents a novel approach called BoostMVSNeRFs to enhance the rendering quality of MVS-based NeRFs in large-scale scenes, and identifies limitations in MVS-based NeRF methods, such as restricted viewport coverage and artifacts due to limited input views.
                              </td>
                            </tr>
                  
                  
                            <tr bgcolor="#ffffff" class="publication paper-Segmentation-Grouping-and-Shape-Analysis paper-Vision-Language-and-Reasoning">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <br>
                                  <div class="two"><img src='images/cvpr-jjwu24.jpg' width="160"></div>
                                  <img src='images/cvpr-jjwu24.jpg' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://arxiv.org/abs/2404.04231">
                                  <papertitle>Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation</papertitle>
                                </a>
                                <br>
                                <a href="https://github.com/072jiajia">Ji-Jia Wu</a>, 
                                Andy Chia-Hao Chang, 
                                Chieh-Yu Chuang, 
                                Chun-Pei Chen, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="https://minhungchen.netlify.app/">Min-Hung Chen</a>, 
                                <a href="https://eborboihuc.github.io/">Hou-Ning Hu</a>, 
                                <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
                                <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>
                                <br>
                                <em>CVPR</em>, 2024 &nbsp
                                <br>
                                <a href="https://arxiv.org/abs/2404.04231">arXiv</a>
                                /
                                <a href="https://github.com/072jiajia/image-text-co-decomposition">code</a>
                                <p></p>
                                <p>
                                  This paper addresses text-supervised semantic segmentation, aiming to learn a model capable of segmenting arbitrary visual concepts within images by using only imagetext pairs without dense annotations.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Ma2024HNRstop()" onmouseover="Ma2024HNR_start()"  bgcolor="#ffffff" class="publication paper-Humans-Face-Body-Pose-Gesture-Movement paper-Humans-Face-Body-Pose-Gesture-Movement">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Ma2024HNR_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Ma2024HNR.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                    <br><img src='images/Ma2024HNR.jpg' width="160" alt="Description" style="margin-top: 20px;">
                                </div>
                                <script type="text/javascript">
                                  function Ma2024HNR_start() {
                                    document.getElementById('Ma2024HNR_image').style.opacity = "1";
                                  }
                  
                                  function Ma2024HNRstop() {
                                    document.getElementById('Ma2024HNR_image').style.opacity = "0";
                                  }
                                  Ma2024HNRstop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://miles629.github.io/humannerf-se.github.io/">
                                  <papertitle>HumanNeRF-SE: A Simple yet Effective Approach to Animate HumanNeRF with Diverse Poses</papertitle>
                                </a>
                                <br>
                                Caoyuan Ma,
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="https://lightchaserx.github.io/">Zhixiang Wang</a>, 
                                <a href="http://liuwu.weebly.com/">Wu Liu</a>, 
                                <a href="https://xinchenliu.com/">Xinchen Liu</a>, 
                                <a href="https://wangzwhu.github.io/home/">Zheng Wang</a>
                                <br>
                                <em>CVPR</em>, 2024 &nbsp
                                <br>
                                <a href="https://miles629.github.io/humannerf-se.github.io/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2312.02232">arXiv</a>
                                /
                                <a href="https://github.com/Miles629/HumanNeRF-SE">code</a>
                                <p></p>
                                <p>
                                  This work reconstructs the previous HumanNeRF approach, combining explicit and implicit human representations with both general and specific mapping processes, and shows that explicit shape can filter the information used to fit implicit representation, and frozen general mapping combined with point-specific mapping can effectively avoid overfitting and improve pose generalization performance.
                              </td>
                            </tr>
                  
                            <tr bgcolor="#ffffff" class="publication paper-Humans-Face-Body-Pose-Gesture-Movement paper-Low-level-Vision">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <br>
                                  <div class="two"><img src='images/daefr_teaser.jpg' width="160"></div>
                                  <img src='images/daefr_teaser.jpg' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://liagm.github.io/DAEFR/">
                                  <papertitle>Dual Associated Encoder for Face Restoration</papertitle>
                                </a>
                                <br>
                                <a href="https://liagm.github.io/">Yu-Ju Tsai</a>, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="http://luqi.info/">Lu Qi</a>, 
                                <a href="https://ckkelvinchan.github.io/">Kelvin C.K. Chan</a>, 
                                <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
                                <br>
                                <em>ICLR</em>, 2024 &nbsp
                                <br>
                                <a href="https://liagm.github.io/DAEFR/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2308.07314">arXiv</a>
                                <p></p>
                                <p>
                                  This work proposes a novel dual-branch framework named DAEFR, which introduces an auxiliary LQ branch that extracts crucial information from the LQ inputs and incorporates association training to promote effective synergy between the two branches, enhancing code prediction and output quality.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Wang2023DCOstop()" onmouseover="Wang2023DCO_start()"  bgcolor="#ffffff" class="publication paper-Humans-Face-Body-Pose-Gesture-Movement paper-Image-and-Video-Synthesis-and-Generation">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <!-- <div class="two" id='Wang2023DCO_image'><img src='images/Wang2023DCO.gif' width="160"></div>
                                  <img src='images/Wang2023DCO.jpg' width="160">
                                </div> -->
                                  <div class="two" id='Wang2023DCO_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Wang2023DCO.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                  <img src='images/Wang2023DCO.jpg' width="160" alt="Description" style="margin-top: 20px;">
                                </div>
                                <script type="text/javascript">
                                  function Wang2023DCO_start() {
                                    document.getElementById('Wang2023DCO_image').style.opacity = "1";
                                  }
                  
                                  function Wang2023DCOstop() {
                                    document.getElementById('Wang2023DCO_image').style.opacity = "0";
                                  }
                                  Wang2023DCOstop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://portrait-disco.github.io/">
                                  <papertitle>DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs</papertitle>
                                </a>
                                <br>
                                <a href="https://lightchaserx.github.io/">Zhixiang Wang</a>, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>, 
                                <a href="http://www.satoh-lab.nii.ac.jp/index.html">Shin'ichi Satoh</a>, 
                                <a href="https://sizhuoma.netlify.app/">Sizhuo Ma</a>, 
                                <a href="https://www.linkedin.com/in/krishnanguru/">Guru Krishnan</a>, 
                                <a href="https://jianwang-cmu.github.io/">Jian Wang</a>
                                <br>
                                <em>IJCV</em>, 2024 &nbsp
                                <br>
                                <a href="https://portrait-disco.github.io/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2302.12253">arXiv</a>
                                <p></p>
                                <p>
                                  This work proposes a simple yet effective method for correcting perspective distortions in a single close-up face using GAN inversion using a perspective-distorted input facial image, and develops starting from a short distance, optimization scheduling, reparametrizations, and geometric regularization.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Cheng2024IRJstop()" onmouseover="Cheng2024IRJ_start()"  bgcolor="#ffffd0" class="publication paper-Image-and-Video-Synthesis-and-Generation paper-Image-and-Video-Synthesis-and-Generation paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Cheng2024IRJ_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Cheng2024IRJ.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                    <br><img src='images/aaai24_jointtensorf.jpg' width="160" alt="Description" style="margin-top: 20px;">
                                </div>
                                <script type="text/javascript">
                                  function Cheng2024IRJ_start() {
                                    document.getElementById('Cheng2024IRJ_image').style.opacity = "1";
                                  }
                  
                                  function Cheng2024IRJstop() {
                                    document.getElementById('Cheng2024IRJ_image').style.opacity = "0";
                                  }
                                  Cheng2024IRJstop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://alex04072000.github.io/Joint-TensoRF/">
                                  <papertitle>Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields</papertitle>
                                </a>
                                <br>
                                Bo-Yu Cheng, 
                                <a href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, 
                                <strong>Yu-Lun Liu</strong>
                                <br>
                                <em>AAAI</em>, 2024 &nbsp
                                <br>
                                <a href="https://alex04072000.github.io/Joint-TensoRF/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2402.13252">arXiv</a>
                                /
                                <a href="https://github.com/Nemo1999/Joint-TensoRF">code</a>
                                <p></p>
                                <p>
                                  An algorithm that allows joint refinement of camera pose and scene geometry represented by decomposed low-rank tensor, using only 2D images as supervision is proposed, which achieves an equivalent effect to brute-force 3D convolution with only incurring little computational overhead.
                              </td>
                            </tr>

                            <!-- Year Row -->
                            <tr class="year-group">
                              <td colspan="2" class="year">
                                <br><center><subsubheading>2023</subsubheading></center>
                              </td>
                            </tr>
                  
                            <tr bgcolor="#ffffff" class="publication paper-Image-and-Video-Synthesis-and-Generation paper-Low-level-Vision">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two"><img src='images/iccv-chen23.jpg' width="160"></div>
                                  <img src='images/iccv-chen23.jpg' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://skchen1993.github.io/CEVR_web/">
                                  <papertitle>Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction</papertitle>
                                </a>
                                <br>
                                Su-Kai Chen, 
                                Hung-Lin Yen, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="https://minhungchen.netlify.app/">Min-Hung Chen</a>, 
                                <a href="https://eborboihuc.github.io/">Hou-Ning Hu</a>, 
                                <a href="https://sites.google.com/g2.nctu.edu.tw/wpeng">Wen-Hsiao Peng</a>, 
                                <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>
                                <br>
                                <em>ICCV</em>, 2023 &nbsp
                                <br>
                                <a href="https://skchen1993.github.io/CEVR_web/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2309.03900">arXiv</a>
                                /
                                <a href="https://github.com/skchen1993/2023_CEVR">code</a>
                                /
                                <a href="https://www.youtube.com/watch?v=Az8W2lGegcg">video</a>
                                <p></p>
                                <p>
                                  This work proposes the continuous exposure value representation (CEVR), which uses an implicit function to generate LDR images with arbitrary EVs, including those unseen during training, to improve HDR reconstruction.
                              </td>
                            </tr>
                            
                  
                            <tr onmouseout="Tu2023IGNstop()" onmouseover="Tu2023IGN_start()"  bgcolor="#ffffff" class="publication paper-3D-from-Multi-view-and-Sensors paper-3D-from-Multi-view-and-Sensors">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two" id='Tu2023IGN_image'><img src='images/imgeonet.gif' width="160"></div>
                                  <img src='images/imgeonet.jpg' width="160">
                                </div>
                                <script type="text/javascript">
                                  function Tu2023IGN_start() {
                                    document.getElementById('Tu2023IGN_image').style.opacity = "1";
                                  }
                  
                                  function Tu2023IGNstop() {
                                    document.getElementById('Tu2023IGN_image').style.opacity = "0";
                                  }
                                  Tu2023IGNstop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://ttaoretw.github.io/imgeonet/">
                                  <papertitle>ImGeoNet: Image-induced Geometry-aware Voxel Representation for Multi-view 3D Object Detection</papertitle>
                                </a>
                                <br>
                                <a href="https://ttaoretw.github.io/">Tao Tu</a>, 
                                <a href="https://www.linkedin.com/in/shunpo">Shun-Po Chuang</a>, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="https://sunset1995.github.io/">Cheng Sun</a>, 
                                Ke Zhang, 
                                Donna Roy, 
                                Cheng-Hao Kuo, 
                                <a href="https://aliensunmin.github.io/">Min Sun</a>
                                <br>
                                <em>ICCV</em>, 2023 &nbsp
                                <br>
                                <a href="https://ttaoretw.github.io/imgeonet/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2308.09098">arXiv</a>
                                <p></p>
                                <p>
                                  The studies indicate that the proposed image-induced geometry-aware representation can enable image-based methods to attain superior detection accuracy than the seminal point cloud-based method, VoteNet, in two practical scenarios: (1) scenarios where point clouds are sparse and noisy, such as in ARKitScenes, and (2) scenarios involve diverse object classes, particularly classes of small objects, as in the case in ScanNet200.
                              </td>
                            </tr>
                  
                  
                            <tr onmouseout="Meuleman2023POLstop()" onmouseover="Meuleman2023POL_start()"  bgcolor="#ffffff" class="publication paper-3D-from-Multi-view-and-Sensors paper-3D-from-Multi-view-and-Sensors">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <!-- <div class="two" id='Meuleman2023POL_image'><img src='images/Meuleman2023POL.gif' width="160"></div>
                                  <img src='images/Meuleman2023POL.jpg' width="160" height="90">
                                </div> -->
                                  <div class="two" id='Meuleman2023POL_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Meuleman2023POL.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                  <img src='images/Meuleman2023POL.jpg' width="160" alt="Description" style="margin-top: 35px;">
                                </div>
                                <script type="text/javascript">
                                  function Meuleman2023POL_start() {
                                    document.getElementById('Meuleman2023POL_image').style.opacity = "1";
                                  }
                  
                                  function Meuleman2023POLstop() {
                                    document.getElementById('Meuleman2023POL_image').style.opacity = "0";
                                  }
                                  Meuleman2023POLstop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://localrf.github.io/">
                                  <papertitle>Progressively Optimized Local Radiance Fields for Robust View Synthesis</papertitle>
                                </a>
                                <br>
                                <a href="https://ameuleman.github.io/">Andreas Meuleman</a>, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="http://chengao.vision/">Chen Gao</a>, 
                                <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>, 
                                <a href="https://changilkim.com/">Changil Kim</a>, 
                                <a href="http://vclab.kaist.ac.kr/minhkim/">Min H. Kim</a>, 
                                <a href="http://johanneskopf.de/">Johannes Kopf</a>
                                <br>
                                <em>CVPR</em>, 2023 &nbsp
                                <br>
                                <a href="https://localrf.github.io/">project page</a>
                                /
                                <a href="https://localrf.github.io/localrf.pdf">paper</a>
                                /
                                <a href="https://github.com/facebookresearch/localrf">code</a>
                                /
                                <a href="https://www.youtube.com/watch?v=GfXAHDxUY4M">video</a>
                                <p></p>
                                <p>
                                  This work presents an algorithm for reconstructing the radiance field of a large-scale scene from a single casually captured video, and shows that progressive optimization significantly improves the robustness of the reconstruction.
                              </td>
                            </tr>
                  
                        
                          <tr onmouseout="Liu2023RDR_stop()" onmouseover="Liu2023RDR_start()"  bgcolor="#ffffd0" class="publication paper-Image-and-Video-Synthesis-and-Generation paper-3D-from-Multi-view-and-Sensors paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <!-- <div class="two" id='Liu2023RDR_image'><img src='images/Liu2023RDR.gif' width="160"></div>
                                  <img src='images/Liu2023RDR.jpg' width="160" height="90"> -->
                                  <div class="two" id='Liu2023RDR_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/Liu2023RDR.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                  <img src='images/Liu2023RDR.jpg' width="160" alt="Description" style="margin-top: 35px;">
                                </div>
                                <script type="text/javascript">
                                  function Liu2023RDR_start() {
                                    document.getElementById('Liu2023RDR_image').style.opacity = "1";
                                  }
                  
                                  function Liu2023RDR_stop() {
                                    document.getElementById('Liu2023RDR_image').style.opacity = "0";
                                  }
                                  Liu2023RDR_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://robust-dynrf.github.io/">
                                  <papertitle>Robust Dynamic Radiance Fields</papertitle>
                                </a>
                                <br>
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="http://chengao.vision/">Chen Gao</a>, 
                                <a href="https://ameuleman.github.io/">Andreas Meuleman</a>, 
                                <a href="https://hytseng0509.github.io/">Hung-Yu Tseng</a>, 
                                <a href="https://scholar.google.com/citations?user=bluhHm8AAAAJ&hl=en">Ayush Saraf</a>, 
                                <a href="https://changilkim.com/">Changil Kim</a>, 
                                <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
                                <a href="http://johanneskopf.de/">Johannes Kopf</a>, 
                                <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>
                                <br>
                                <em>CVPR</em>, 2023 &nbsp
                                <br>
                                <a href="https://robust-dynrf.github.io/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2301.02239">arXiv</a>
                                /
                                <a href="https://github.com/facebookresearch/robust-dynrf">code</a>
                                /
                                <a href="https://www.youtube.com/watch?v=38S56ottFQ4">video</a>
                                <p></p>
                                <p>
                                  This work addresses the robustness issue by jointly estimating the static and dynamic radiance fields along with the camera parameters (poses and focal length) and shows favorable performance over the state-of-the-art dynamic view synthesis methods.
                              </td>
                            </tr>

                            <!-- Year Row -->
                            <tr class="year-group">
                              <td colspan="2" class="year">
                                <br><center><subsubheading>2022</subsubheading></center>
                              </td>
                            </tr>
                      
                  
                            <tr bgcolor="#ffffff" class="publication paper-Image-and-Video-Synthesis-and-Generation paper-Image-and-Video-Synthesis-and-Generation">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                            <br>
                            <br>
                            <br>
                                  <div class="two"><img src='images/iclr2022.jpg' width="160"></div>
                                  <img src='images/iclr2022.jpg' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://arxiv.org/abs/2203.14206">
                                  <papertitle>Denoising Likelihood Score Matching for Conditional Score-based Data Generation</papertitle>
                                </a>
                                <br>
                                Chen-Hao Chao, 
                                <a href="https://scholar.google.com.tw/citations?user=TgMlVRUAAAAJ&hl=zh-TW">Wei-Fang Sun</a>, 
                                Bo-Wun Cheng, 
                                <a href="https://scholar.google.com/citations?user=EPYQ48sAAAAJ&hl=zh-TW">Yi-Chen Lo</a>, 
                                <a href="https://scholar.google.com.tw/citations?user=FK1RcpoAAAAJ&hl=zh-TW">Chia-Che Chang</a>, 
                                <strong>Yu-Lun Liu</strong>, 
                                <a href="https://scholar.google.com/citations?user=0O9rukQAAAAJ&hl=en">Yu-Lin Chang</a>, 
                                <a href="https://tw.linkedin.com/in/chia-ping-chen-81674078">Chia-Ping Chen</a>, 
                                <a href="https://elsalab.ai/about">Chun-Yi Lee</a>
                                <br>
                                <em>ICLR</em>, 2022 &nbsp
                                <br>
                                <a href="https://arxiv.org/abs/2203.14206">arXiv</a>
                                /
                                <a href="https://openreview.net/forum?id=LcF-EEt8cCC">OpenReview</a>
                                <p></p>
                                <p>
                                  This work forms a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density, and concludes that the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated.
                              </td>
                            </tr>

                            <!-- Year Row -->
                            <tr class="year-group">
                              <td colspan="2" class="year">
                                <br><center><subsubheading>2021</subsubheading></center>
                              </td>
                            </tr>
                  
                        
                            <tr onmouseout="Liu2021SOLD_stop()" onmouseover="Liu2021SOLD_start()"  bgcolor="#ffffd0" class="publication paper-Low-level-Vision paper-Video-Low-level-Analysis-Motion-and-Tracking paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <!-- <div class="two" id='Liu2021SOLD_image'><img src='images/cvpr20_reflection.gif' width="160"></div>
                                  <img src='images/pamiobstruction.jpg' width="160" height="90">
                                </div> -->
                                <div class="two" id='Liu2021SOLD_image'><video  width=100% height=100% muted autoplay loop>
                                  <source src="images/cvpr20_reflection.mp4" type="video/mp4">
                                  Your browser does not support the video tag.
                                  </video></div>
                                <img src='images/pamiobstruction.jpg' width="160" alt="Description" style="margin-top: 35px;" height="90">
                              </div>
                                <script type="text/javascript">
                                  function Liu2021SOLD_start() {
                                    document.getElementById('Liu2021SOLD_image').style.opacity = "1";
                                  }
                  
                                  function Liu2021SOLD_stop() {
                                    document.getElementById('Liu2021SOLD_image').style.opacity = "0";
                                  }
                                  Liu2021SOLD_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://alex04072000.github.io/SOLD/">
                                  <papertitle>Learning to See Through Obstructions with Layered Decomposition</papertitle>
                                </a>
                                <br>
                          <strong>Yu-Lun Liu</strong>, 
                                <a href="https://www.wslai.net/">Wei-Sheng Lai</a>, 
                                <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, 
                                <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
                                <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a>
                                <br>
                                <em>TPAMI</em>, 2021 &nbsp
                                <br>
                                <a href="https://alex04072000.github.io/SOLD/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2008.04902">arXiv</a>
                                /
                                <a href="https://github.com/alex04072000/SOLD">code</a>
                                /
                                <a href="https://colab.research.google.com/drive/1kCG5SJd3usgzi6Bx979KiaO_YTanNVVz?usp=sharing">demo</a>
                                /
                                <a href="https://www.youtube.com/watch?v=oqdvYRYOT5s&t=4s">video</a>
                                <p></p>
                                <p>
                                  This work alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network, facilitates accommodating potential errors in the flow estimation and brittle assumptions, such as brightness consistency.
                              </td>
                            </tr>
                      
                        <tr bgcolor="#ffffff" class="publication paper-Physics-based-Vision-and-Shape-from-X paper-Physics-based-Vision-and-Shape-from-X">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                            <br>
                            <br>
                            <br>
                                  <div class="two"><img src='images/iccv2021.jpg' width="160"></div>
                                  <img src='images/iccv2021.jpg' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://albert100121.github.io/AiFDepthNet/">
                                  <papertitle>Bridging Unsupervised and Supervised Depth from Focus via All-in-Focus Supervision</papertitle>
                                </a>
                                <br>
                                <a href="http://albert100121.github.io/">Ning-Hsu Wang</a>,
                                <a href="https://tw.linkedin.com/in/ren-wang-61b273160">Ren Wang</a>,
                          <strong>Yu-Lun Liu</strong>, 
                                <a href="https://www.linkedin.com/in/yu-hao-huang-72821060/">Yu-Hao Huang</a>, 
                                <a href="https://scholar.google.com/citations?user=0O9rukQAAAAJ&hl=en">Yu-Lin Chang</a>, 
                                <a href="https://tw.linkedin.com/in/chia-ping-chen-81674078">Chia-Ping Chen</a>, 
                                <a href="https://dblp.org/pid/160/5531.html">Kevin Jou</a>
                                <br>
                                <em>ICCV</em>, 2021 &nbsp
                                <br>
                                <a href="https://albert100121.github.io/AiFDepthNet/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2108.10843">arXiv</a>
                                /
                                <a href="https://github.com/albert100121/AiFDepthNet">code</a>
                                <p></p>
                                <p>
                                  This paper proposes a method to estimate not only a depth map but an AiF image from a set of images with different focus positions (known as a focal stack), and shows that this method outperforms the state-of-the-art methods both quantitatively and qualitatively, and also has higher efficiency in inference time.
                              </td>
                            </tr>
                      
                        <tr onmouseout="Liu2021HNF_stop()" onmouseover="Liu2021HNF_start()"  bgcolor="#ffffd0" class="publication paper-Video-Low-level-Analysis-Motion-and-Tracking paper-Video-Low-level-Analysis-Motion-and-Tracking paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <!-- <div class="two" id='Liu2021HNF_image'><br><img src='images/iccv21_nervis.gif' width="160"></div>
                                  <img src='images/FuSta.jpg' width="160" id='fusta_image'>
                                </div> -->
                                <div class="two" id='Liu2021HNF_image'><video  width=100% height=100% muted autoplay loop>
                                  <source src="images/iccv21_nervis.mp4" type="video/mp4">
                                  Your browser does not support the video tag.
                                  </video></div>
                                <img src='images/FuSta.jpg' width="160" alt="Description" style="margin-top: 8px;" id='fusta_image'>
                              </div>
                                <script type="text/javascript">
                                  function Liu2021HNF_start() {
                                    document.getElementById('Liu2021HNF_image').style.opacity = "1";
                                    document.getElementById('fusta_image').style.opacity = "0";
                                  }
                  
                                  function Liu2021HNF_stop() {
                                    document.getElementById('Liu2021HNF_image').style.opacity = "0";
                                    document.getElementById('fusta_image').style.opacity = "1";
                                  }
                                  Liu2021HNF_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://alex04072000.github.io/FuSta/">
                                  <papertitle>Hybrid Neural Fusion for Full-frame Video Stabilization</papertitle>
                                </a>
                                <br>
                          <strong>Yu-Lun Liu</strong>, 
                                <a href="https://www.wslai.net/">Wei-Sheng Lai</a>, 
                                <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, 
                                <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
                                <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a>
                                <br>
                                <em>ICCV</em>, 2021 &nbsp
                                <br>
                                <a href="https://alex04072000.github.io/FuSta/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2102.06205">arXiv</a>
                                /
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/FuSta/iccv21_poster.pdf">poster</a>
                                /
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/FuSta/slides_mtk_25min.pptx">slides</a>
                                /
                                <a href="https://github.com/alex04072000/FuSta">code</a>
                                /
                                <a href="https://colab.research.google.com/drive/1l-fUzyM38KJMZyKMBWw_vu7ZUyDwgdYH?usp=sharing">demo</a>
                                /
                                <a href="https://www.youtube.com/watch?v=KO3sULs4hso">video</a>
                                /
                                <a href="https://www.youtube.com/watch?v=v5pOsQEOsyA">Two minute video</a>
                                <p></p>
                                <p>
                                  This work presents a frame synthesis algorithm to achieve full-frame video stabilization that first estimate dense warp fields from neighboring frames and then synthesize the stabilized frame by fusing the warped contents.
                              </td>
                            </tr>

                            <!-- Year Row -->
                            <tr class="year-group">
                              <td colspan="2" class="year">
                                <br><center><subsubheading>2020</subsubheading></center>
                              </td>
                            </tr>
                  
                            <tr bgcolor="#ffffff" class="publication paper-Computational-Imaging paper-Image-and-Video-Synthesis-and-Generation">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                            <br>
                            <br>
                                  <div class="two"><img src='images/icpr2020.png' width="160"></div>
                                  <img src='images/icpr2020.png' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://arxiv.org/abs/2010.10000">
                                  <papertitle>Explorable Tone Mapping Operators</papertitle>
                                </a>
                                <br>
                                Chien-Chuan Su, 
                                <a href="https://tw.linkedin.com/in/ren-wang-61b273160">Ren Wang</a>,
                                <a href="https://github.com/leVirve">Hung-Jin Lin</a>, 
                          <strong>Yu-Lun Liu</strong>, 
                                <a href="https://tw.linkedin.com/in/chia-ping-chen-81674078">Chia-Ping Chen</a>, 
                                <a href="https://scholar.google.com/citations?user=0O9rukQAAAAJ&hl=en">Yu-Lin Chang</a>, 
                                <a href="https://scholar.google.com/citations?user=-JiGrnAAAAAJ">Soo-Chang Pei</a>
                                <br>
                                <em>ICPR</em>, 2020 &nbsp
                                <br>
                                <a href="https://arxiv.org/abs/2010.10000">arXiv</a>
                                <p></p>
                                <p>
                                  This paper proposes a learning-based multimodal tone-mapping method, which not only achieves excellent visual quality but also explores the style diversity and shows that the proposed method performs favorably against state-of-the-art tone-Mapping algorithms both quantitatively and qualitatively.
                        </td>
                            </tr>
                  
                            <tr bgcolor="#ffffff" class="publication paper-Computational-Imaging paper-Low-level-Vision">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                            <br>
                                  <div class="two"><img src='images/eccv2020.png' width="160"></div>
                                  <img src='images/eccv2020.png' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://arcchang1236.github.io/CA-NoiseGAN/">
                                  <papertitle>Learning Camera-Aware Noise Models</papertitle>
                                </a>
                                <br>
                                <a href="http://arcchang1236.github.io/">Ke-Chi Chang</a>,
                                <a href="https://tw.linkedin.com/in/ren-wang-61b273160">Ren Wang</a>,
                                <a href="https://github.com/leVirve">Hung-Jin Lin</a>, 
                          <strong>Yu-Lun Liu</strong>, 
                                <a href="https://tw.linkedin.com/in/chia-ping-chen-81674078">Chia-Ping Chen</a>, 
                                <a href="https://scholar.google.com/citations?user=0O9rukQAAAAJ&hl=en">Yu-Lin Chang</a>, 
                                <a href="https://htchen.github.io/">Hwann-Tzong Chen</a>
                                <br>
                                <em>ECCV</em>, 2020 &nbsp
                                <br>
                                <a href="https://arcchang1236.github.io/CA-NoiseGAN">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2008.09370">arXiv</a>
                                /
                                <a href="https://github.com/arcchang1236/CA-NoiseGAN">code</a>
                                <p></p>
                                <p>
                                  A data-driven approach, where a generative noise model is learned from real-world noise, which is camera-aware and quantitatively and qualitatively outperforms existing statistical noise models and learning-based methods.
                        </td>
                            </tr>
                  
                            <tr onmouseout="Liu2020SID_stop()" onmouseover="Liu2020SID_start()"  bgcolor="#ffffd0" class="publication paper-Computational-Imaging paper-Low-level-Vision paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <!-- <div class="two" id='Liu2020SID_image'><img src='images/cvpr20_hdr.gif' width="160"></div>
                                  <img src='images/shdr.jpg' width="160">
                                </div> -->
                                <div class="two" id='Liu2020SID_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/cvpr20_hdr.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                  <img src='images/shdr.jpg' width="160" alt="Description" style="margin-top: 20px;">
                                </div>
                                <script type="text/javascript">
                                  function Liu2020SID_start() {
                                    document.getElementById('Liu2020SID_image').style.opacity = "1";
                                  }
                  
                                  function Liu2020SID_stop() {
                                    document.getElementById('Liu2020SID_image').style.opacity = "0";
                                  }
                                  Liu2020SID_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://alex04072000.github.io/SingleHDR/">
                                  <papertitle>Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline</papertitle>
                                </a>
                                <br>
                          <strong>Yu-Lun Liu*</strong>, 
                                <a href="https://www.wslai.net/">Wei-Sheng Lai*</a>, 
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~nothinglo/">Yu-Sheng Chen</a>, Yi-Lung Kao, 
                                <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, 
                                <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
                                <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a>
                                <br>
                                <em>CVPR</em>, 2020 &nbsp
                                <br>
                                <a href="https://alex04072000.github.io/SingleHDR/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2004.01179">arXiv</a>
                                /
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/SingleHDR_/cvpr20_singleHDR_poster_mtk.pdf">poster</a>
                                /
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/SingleHDR_/cvpr20_singleHDR_video_slides_15min.pptx">slides</a>
                                /
                                <a href="https://github.com/alex04072000/SingleHDR">code</a>
                                /
                                <a href="https://colab.research.google.com/drive/1WzNaGSaucF2AMDSdUCBMEOauBg4IowMa">demo</a>
                                /
                                <a href="https://www.youtube.com/watch?v=UCHDhk6fciY">1-minute video</a>
                                <p></p>
                                <p>
                                  This work model the HDR-to-LDR image formation pipeline as the dynamic range clipping, non-linear mapping from a camera response function, and quantization, and proposes to learn three specialized CNNs to reverse these steps.
                              </td>
                            </tr>
                  
                            <tr onmouseout="Liu2020LST_stop()" onmouseover="Liu2020LST_start()"  bgcolor="#ffffd0" class="publication paper-Low-level-Vision paper-Video-Low-level-Analysis-Motion-and-Tracking paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <!-- <div class="two" id='Liu2020LST_image'><img src='images/cvpr20_reflection.gif' width="160"></div>
                                  <img src='images/reflection.jpg' width="160">
                                </div> -->
                                <div class="two" id='Liu2020LST_image'><video  width=100% height=100% muted autoplay loop>
                                  <source src="images/cvpr20_reflection.mp4" type="video/mp4">
                                  Your browser does not support the video tag.
                                  </video></div>
                                <img src='images/reflection.jpg' width="160" alt="Description" style="margin-top: 35px;">
                              </div>
                                <script type="text/javascript">
                                  function Liu2020LST_start() {
                                    document.getElementById('Liu2020LST_image').style.opacity = "1";
                                  }
                  
                                  function Liu2020LST_stop() {
                                    document.getElementById('Liu2020LST_image').style.opacity = "0";
                                  }
                                  Liu2020LST_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://alex04072000.github.io/ObstructionRemoval/">
                                  <papertitle>Learning to See Through Obstructions</papertitle>
                                </a>
                                <br>
                          <strong>Yu-Lun Liu</strong>, 
                                <a href="https://www.wslai.net/">Wei-Sheng Lai</a>, 
                                <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, 
                                <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
                                <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a>
                                <br>
                                <em>CVPR</em>, 2020 &nbsp
                                <br>
                                <a href="https://alex04072000.github.io/ObstructionRemoval/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2004.01180">arXiv</a>
                                /
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/ObstructionRemoval_/cvpr20_obstructionRemoval_poster_mtk.pdf">poster</a>
                                /
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/ObstructionRemoval_/cvpr20_obstructionRemoval_video_slides_15min.pptx">slides</a>
                                /
                                <a href="https://github.com/alex04072000/ObstructionRemoval">code</a>
                                /
                                <a href="https://colab.research.google.com/drive/1iOKknc0dePekUH2TEh28EhcRPCS1mgwz">demo</a>
                                /
                                <a href="https://www.youtube.com/watch?v=pJWcHhofYTE">1-minute video</a>
                                /
                                <a href="https://www.youtube.com/watch?v=ICr6xi9wA94">video</a>
                                /
                                <a href="https://www.newscientist.com/article/2253195-ai-removes-unwanted-objects-from-photos-to-give-a-clearer-view/">New Scientists</a>
                                <p></p>
                                <p>
                                  The method leverages the motion differences between the background and the obstructing elements to recover both layers and alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network.
                              </td>
                            </tr>
                  
                            <tr bgcolor="#ffffd0" class="publication paper-3D-from-Multi-view-and-Sensors paper-3D-from-Multi-view-and-Sensors paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                            <br>
                                  <div class="two"><img src='images/AVS.jpg' width="160"></div>
                                  <img src='images/AVS.jpg' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Tsai2020AVS.pdf">
                                  <papertitle>Attention-based View Selection Networks for Light-field Disparity Estimation</papertitle>
                                </a>
                                <br>
                          Yu-Ju Tsai*, 
                          <strong>Yu-Lun Liu*</strong>, 
                                <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, 
                                <a href="https://www.csie.ntu.edu.tw/~ming/">Ming Ouhyoung</a>
                                <br>
                                <em>AAAI</em>, 2020 &nbsp
                                <br>
                                <a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Tsai2020AVS.pdf">paper</a>
                                /
                                <a href="https://github.com/LIAGM/LFattNet">code</a>
                          /
                          <a href="https://lightfield-analysis.uni-konstanz.de/benchmark/table">benchmark</a>
                                <p></p>
                                <p>
                                  A novel deep network for estimating depth maps from a light field image that generates an attention map indicating the importance of each view and its potential for contributing to accurate depth estimation and enforce symmetry in the attention map to improve accuracy.
                              </td>
                            </tr>

                            <!-- Year Row -->
                            <tr class="year-group">
                              <td colspan="2" class="year">
                                <br><center><subsubheading>2019</subsubheading></center>
                              </td>
                            </tr>
                      
                            <tr onmouseout="cyclic_stop()" onmouseover="cyclic_start()"  bgcolor="#ffffd0" class="publication paper-Image-and-Video-Synthesis-and-Generation paper-Image-and-Video-Synthesis-and-Generation paperhi">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <!-- <div class="two" id='cyclic_image'><img src='images/aaai-liu19.gif' width="160"></div>
                          <br>
                                  <img src='images/cyclic.jpg' width="160"> -->
                                  <div class="two" id='cyclic_image'><video  width=100% height=100% muted autoplay loop>
                                    <source src="images/aaai-liu19.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                    <!-- <br> -->
                                  <img src='images/cyclic.jpg' width="160" alt="Description" style="margin-top: 20px;">
                                </div>
                                <script type="text/javascript">
                                  function cyclic_start() {
                                    document.getElementById('cyclic_image').style.opacity = "1";
                                  }
                  
                                  function cyclic_stop() {
                                    document.getElementById('cyclic_image').style.opacity = "0";
                                  }
                                  cyclic_stop()
                                </script>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/CyclicGen">
                                  <papertitle>Deep Video Frame Interpolation using Cyclic Frame Generation</papertitle>
                                </a>
                                <br>
                          <strong>Yu-Lun Liu</strong>, 
                                <a href="http://www.cmlab.csie.ntu.edu.tw/~queenieliaw/">Yi-Tung Liao</a>, 
                                <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>, 
                                <a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>
                                <br>
                                <em>AAAI</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                                <br>
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/CyclicGen">project page</a>
                                /
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/CyclicGen_/liu.pdf">paper</a>
                                /
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/CyclicGen_/aaai19_cyclic_gen_poster.pdf">poster</a>
                                /
                                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/CyclicGen_/CyclicGen_yylin_20190127.pptx">slides</a>
                                /
                                <a href="https://github.com/alex04072000/CyclicGen">code</a>
                                /
                                <a href="https://www.youtube.com/watch?v=R8vQjgAtPOE">video</a>
                                <p></p>
                                <p>
                                  A new loss term, the cycle consistency loss, which can better utilize the training data to not only enhance the interpolation results, but also maintain the performance better with less training data is introduced.
                              </td>
                            </tr>

                            <!-- Year Row -->
                            <tr class="year-group">
                              <td colspan="2" class="year">
                                <br><center><subsubheading>2014</subsubheading></center>
                              </td>
                            </tr>
                        
                        <tr bgcolor="#ffffff" class="publication paper-Low-level-Vision paper-Segmentation-Grouping-and-Shape-Analysis">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two"><img src='images/14.png' width="160"></div>
                                  <img src='images/14.png' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://ieeexplore.ieee.org/abstract/document/7041517/">
                                  <papertitle>Background modeling using depth information</papertitle>
                                </a>
                                <br>
                          <strong>Yu-Lun Liu</strong>, 
                                <a href="https://mcube.nctu.edu.tw/~hmhang/">Hsueh-Ming Hang</a>
                                <br>
                                <em>APSIPA</em>, 2014 &nbsp
                                <br>
                                <a href="http://cwww.ee.nctu.edu.tw/~hmhang/publications/Conf/Background%20Modeling%20Using%20Depth%20Information.pdf">paper</a>
                                <p></p>
                                <p>
                                  This paper focuses on creating a global background model of a video sequence using the depth maps together with the RGB pictures, and develops a recursive algorithm that iterates between the depth map and color pictures.
                              </td>
                            </tr>

                            <!-- Year Row -->
                            <tr class="year-group">
                              <td colspan="2" class="year">
                                <br><center><subsubheading>2013</subsubheading></center>
                              </td>
                            </tr>
                        
                        <tr bgcolor="#ffffff" class="publication paper-3D-from-Multi-view-and-Sensors paper-Vision--Graphics">
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <div class="two"><img src='images/13.png' width="160"></div>
                                  <img src='images/13.png' width="160">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://ieeexplore.ieee.org/abstract/document/6737719">
                                  <papertitle>Virtual view synthesis using backward depth warping algorithm</papertitle>
                                </a>
                                <br>
                          Du-Hsiu Li,
                                <a href="https://mcube.nctu.edu.tw/~hmhang/">Hsueh-Ming Hang</a>,
                          <strong>Yu-Lun Liu</strong>
                                <br>
                                <em>PCS</em>, 2013 &nbsp
                                <br>
                                <a href="https://mcube.nctu.edu.tw/~hmhang/publications/Conf/Virtual%20View%20Synthesis%20Using%20Backward%20Depth%20Warping%20Algorithm.pdf">paper</a>
                                <p></p>
                                <p>
                                  A backward warping process is proposed to replace the forward warped process, and the artifacts (particularly the ones produced by quantization) are significantly reduced, so the subjective quality of the synthesized virtual view images is thus much improved.
                              </td>
                            </tr>
                            
                        </div>
                    </div>
                    
                    <!-- Your existing footer content -->
                    
                </td>
            </tr>
          </tbody></table>

            <hr class="soft">
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <center><heading>Miscellanea</heading></center>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
                <td width="75%" valign="center">
                  <a href="https://accv2024.org/organizers/">Publication Chair, ACCV 2024</a>
                  <br>
                  Conference Reviewer: CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, SIGGRAPH, SIGGRAPH Asia, AAAI, IJCAI
                  <br>
                  Journal Reviewer: TPAMI, IJCV, TIP, TOG
                  <!-- <br>
                  <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                  <br>
                  <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                  <br>
                  <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a> -->
                </td>
              </tr>
            </tbody></table>
    
            <hr class="soft">
    
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <center><heading>Teaching</heading></center>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" align="center"><img src="images/VC.jpg" height="50"></td>
                <td width="75%" valign="center">
                  <a href="https://timetable.nycu.edu.tw/?r=main/crsoutline&Acy=113&Sem=1&CrsNo=535657&lang=zh-tw"><papertitle>CSIC30107: Video Compression</papertitle></a>
                  <br>
                  NYCU - Spring 2023, Fall 2023, Fall 2024 (Instructor)
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" align="center"><img src="images/SS.jpg" height="50"></td>
                <td width="75%" valign="center">
                  <a href="https://timetable.nycu.edu.tw/?r=main/crsoutline&Acy=112&Sem=2&CrsNo=515613&lang=zh-tw"><papertitle>CSCS10017: Signals and Systems</papertitle></a>
                  <br>
                  NYCU - Spring 2024 (Instructor)
                </td>
              </tr>
              <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" align="center"><img src="images/VC.jpg" height="50"></td>
                <td width="75%" valign="center">
                  <a href="https://timetable.nycu.edu.tw/?r=main/crsoutline&Acy=112&Sem=1&CrsNo=535659&lang=zh-tw"><papertitle>CSIC30107: Video Compression</papertitle></a>
                  <br>
                  NYCU - Fall 2023 (Instructor)
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" align="center"><img src="images/VC.jpg" height="50"></td>
                <td width="75%" valign="center">
                  <a href="https://timetable.nycu.edu.tw/?r=main/crsoutline&Acy=111&Sem=2&CrsNo=535659&lang=zh-tw"><papertitle>CSIC30107: Video Compression</papertitle></a>
                  <br>
                  NYCU - Spring 2023 (Instructor)
                </td>
              </tr> -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" align="center"><img src="images/PS.jpg" height="50"></td>
                <td width="75%" valign="center">
                  <a href="https://timetable.nycu.edu.tw/?r=main/crsoutline&Acy=101&Sem=2&CrsNo=1017&lang=zh-tw"><papertitle>DEE1315: Probability and Statistics</papertitle></a>
                  <br>
                  NCTU - Spring 2013 (Teaching Assistant)
                </td>
              </tr>
            </tbody></table>
    
            <hr class="soft">
    
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <center><heading>Funding</heading></center>
                  <p>My research is made possible by the generous support of the following organizations.
                    <!-- bootstrap -->
                    <!-- <div class="container">
                      <div class="row">
                        <div class="col-xs-2 col-sm-6 col-md-4" align="center"><img src="images/ROC_Ministry_of_Education_Seal.svg.png" height="50"></div>
                        <div class="col-xs-2 col-sm-6 col-md-4" align="center"><img src="images/2880px-MediaTek_logo.svg.png" height="26"></div>
                        <div class="col-xs-2 col-sm-6 col-md-4" align="center"><img src="images/ROC_National_Science_and_Technology_Council.svg.png" height="19"></div>
                      </div> -->
    
                    <!-- three columes for images, each 33% of the talbe size -->
                    <!-- columes! not rows! -->
                    <!-- <div class="container">
                      <div class="row">
                    <div class="col-xs-2 col-sm-6 col-md-4" align="center"><img src="images/2880px-MediaTek_logo.svg.png" width="160"></div>
                    <div class="col-xs-2 col-sm-6 col-md-4" align="center"><img src="images/ROC_National_Science_and_Technology_Council.svg.png" width="160"></div>
                    <div class="col-xs-2 col-sm-6 col-md-4" align="center"><img src="images/2880px-Industrial_Technology_Research_Institute_logo.svg.png" width="160"></div>
                  </div>
                </div> -->
                <!-- <br>
                <br>
                <br> -->
                <!-- <div class="row centered">
                  <div class="sponsor">
                    <p><img src="images/ROC_Ministry_of_Education_Seal.svg.png" width="128"></p>
                  </div>
                  <div class="sponsor">
                    <p><img src="images/2880px-MediaTek_logo.svg.png" width="128"></p>
                  </div>
                  <div class="sponsor">
                    <p><img src="images/ROC_National_Science_and_Technology_Council.svg.png" width="128"></p>
                  </div>
                <div class="row centered">
                </div>
                  <div class="sponsor">
                    <p><img src="images/NVIDIA_logo.svg.png" width="128"></p>
                  </div>
                  <div class="sponsor">
                    <p><img src="images/2880px-Industrial_Technology_Research_Institute_logo.svg.png" width="128"></p>
                  </div>
                </div> -->
    
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr style="padding:0px">
                    <td><img height=50 src="images/ROC_Ministry_of_Education_Seal.svg.png"></td>
                    <td><img height=26 src="images/2880px-MediaTek_logo.svg.png"></td>
                    <td><img height=19 src="images/ROC_National_Science_and_Technology_Council.svg.png"></td>
                    <td><img height=29 src="images/Google_2015_logo.svg.png"></td>
                    <td><img height=22 src="images/NVIDIA_logo.svg.png"></td>
                    <td><img height=36 src="images/500px-National_Taiwan_University_Hospital_logo.svg.png"></td>
                    <td><img height=24 src="images/2880px-Industrial_Technology_Research_Institute_logo.svg.png"></td>
                    <!-- <td style="padding:0px;width:100%;vertical-align:middle">
                      <div class="people">
                        <div class="sponsor">
                          <img height=70 src="images/ROC_Ministry_of_Education_Seal.svg.png" />
                        </div>
                        <div class="sponsor">
                          <img height=36 src="images/2880px-MediaTek_logo.svg.png" />
                        </div>
                        <div class="sponsor">
                          <img height=27 src="images/ROC_National_Science_and_Technology_Council.svg.png" />
                        </div>
                        <div class="sponsor">
                          <img height=41 src="images/Google_2015_logo.svg.png" />
                        </div>
                        <div class="sponsor">
                          <img height=30 src="images/NVIDIA_logo.svg.png" />
                        </div>
                        <div class="sponsor">
                          <img height=50 src="images/500px-National_Taiwan_University_Hospital_logo.svg.png" />
                        </div>
                        <div class="sponsor">
                          <img height=34 src="images/2880px-Industrial_Technology_Research_Institute_logo.svg.png" />
                        </div>
                      </div>
                    </td> -->
                  </tr>
                </tbody></table>
                <!-- &nbsp&nbspÊïôËÇ≤ÈÉ®&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspËÅØÁôºÁßëÊäÄ&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspÂúãÁßëÊúÉ&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspGoogle&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspNVIDIA&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspÂè∞Â§ßÈÜ´Èô¢&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspÂ∑•Á†îÈô¢ -->
    
    
                    <!-- <div style="width: 25%; text-align: center;"><img src="images/2880px-MediaTek_logo.svg.png"></div>
                    <div style="width: 25%; text-align: center;"><img src="images/2880px-MediaTek_logo.svg.png"></div>
                    <div style="width: 25%; text-align: center;"><img src="images/2880px-MediaTek_logo.svg.png"></div> -->
                    <!-- <table><tr><td><img height=50 src="images/2880px-MediaTek_logo.svg.png"></td><td><img height=50 src="images/ROC_National_Science_and_Technology_Council.svg.png"></td><td><img height=50 src="images/2880px-Industrial_Technology_Research_Institute_logo.svg.png"></td></tr></table></p> -->
                </td>
              </tr>
            </tbody></table>
            
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
            Stolen from <a href="https://jonbarron.info/">Jon Barron</a>'s website.
                    <br>
                    Last updated Nov 2024.
            </p>
                </td>
              </tr>
            </tbody></table>
    
            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <heading>Service</heading>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
                <td width="75%" valign="center">
                  Reviewer: 
                  <br>
                  SIGGRAPH 2022
                  <br>
                  CVPR 2022
                  <br>
                  ICCV 2021
                  <br>
                  ECCV 2020-2022
                  <br>
                  ACCV 2020
                  <br>
                  ICLR 2022
                  <br>
                  NeurIPS 2022
                  <br>
                  AAAI 2021-2022
                  <br>
                  IJCAI 2021-2022
                  <br>
                  MM 2018
                  <br>
                  TIP
                  <br>
                  Applied Soft Computing
                  <br>
                  Pattern Recognition
                  <br>
                  CVIU
                  <br>
                  Multidimensional Systems and Signal Processing
                </td>
              </tr>
            </tbody></table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
            Stolen from <a href="https://jonbarron.info/">Jon Barron</a>'s website.
                    <br>
                    Last updated Feburary 2023.
            </p>
                </td>
              </tr>
            </tbody></table> -->
          </td>
        </tr>
      </table>

        </tbody>
    </table>

    <!-- <script>
    function PaperSelect(topic) {
    console.log("Selected topic:", topic);
    
    if (topic == "All") {
        $(".publication").show();
        $(".papers-container .year").show();
        $("#paper-heading").html("All Papers by Year");
    } else if (topic == "Representative") {
        $(".publication").hide();
        $(".paperhi").show();
        $(".papers-container .year").hide();
        $("#paper-heading").html("Representative Papers");
    } else {
        $(".publication").hide();
        var selector = ".paper-" + topic.replace(/ /g, "-").replaceAll(",", "").replaceAll(":", "");
        console.log("Selector:", selector);
        console.log("Matching elements:", $(selector).length);
        $(selector).show();
        $(".papers-container .year").hide();
        $("#paper-heading").html("Papers: " + topic);
    }

    console.log("Visible publications:", $(".publication:visible").length);

    $(".nav-link")
        .removeClass("active")
        .each(function () {
            if ($(this).children("span").html() == topic) {
                $(this).addClass("active");
            }
        });
}


$(document).ready(function () {
    $(".nav-link").click(function () {
        var topic = $(this).children("span").html();
        PaperSelect(topic);
    });

    PaperSelect("Representative");
});


    </script>

    <script xml:space="preserve" language="javascript">hideblock('alumni');</script> -->

    <script>
      function PaperSelect(topic) {
    console.log("Selected topic:", topic);

    if (topic === "All") {
        $(".publication").show();
        $(".year-group .year").show();
        $(".year-group").show();
        $("#paper-heading").html("All Papers by Year");

        // Add 'highlighted' class to representative papers
        $(".paperhi").addClass("highlighted");
    } else if (topic === "Representative") {
        $(".publication").hide();
        $(".paperhi").show();
        $(".year-group .year").hide();
        $("#paper-heading").html("Representative Papers");

        // Remove 'highlighted' class from representative papers
        $(".paperhi").removeClass("highlighted");
    } else {
        $(".publication").hide();
        $(".year-group").hide(); // Hide all year groups initially
        var selector = ".paper-" + topic.replace(/ /g, "-").replace(/[,:\(\)]/g, "").replaceAll("+", "");
        console.log("Selector:", selector);
        console.log("Matching elements:", $(selector).length);
        $(selector).each(function () {
            $(this).show();
            $(this).prevAll(".year-group:first").show(); // Show the preceding year group
        });
        $(".year-group .year").hide();
        $("#paper-heading").html("Papers: " + topic);

        // Add 'highlighted' class to representative papers
        $(".paperhi").addClass("highlighted");
    }

    console.log("Visible publications:", $(".publication:visible").length);

    $(".method-pill")
        .removeClass("active")
        .each(function () {
            if ($(this).data("value") === topic) {
                $(this).addClass("active");
            }
        });
}

$(document).ready(function () {
    $(".method-pill").click(function () {
        var topic = $(this).data("value");
        PaperSelect(topic);
    });

    // Initialize with the "Representative" topic
    PaperSelect("Representative");
});

    </script>
  

<script xml:space="preserve" language="javascript">hideblock('alumni');</script>
</body>
</html>